{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6HYWo0HW1U4mKziQT4w+r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Junoflows/DeepLearning_From_Scratch1/blob/main/Chapter_5_%EC%98%A4%EC%B0%A8%EC%97%AD%EC%A0%84%ED%8C%8C%EB%B2%95.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 5 오차역전파법"
      ],
      "metadata": {
        "id": "-jwUGq_Fd8Dt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 가중치 매개변수의 기울기를 효울적으로 계산하는 방법이다.\n",
        "+ 오차역전파법을 계산 그래프를 통해 시각적으로 이해해보자."
      ],
      "metadata": {
        "id": "xO0mMzkDfQWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 계산 그래프"
      ],
      "metadata": {
        "id": "hMKj1PIlgJjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 계산 그래프는 계산 과정을 그래프로 나타낸 것이다.\n",
        "+ 그래프는 복수의 노드와 에지로 표현된다. (노드 사이의 직선을 에지라고 한다)\n"
      ],
      "metadata": {
        "id": "GWl07Csb-gHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1.1 계산 그래프로 풀다."
      ],
      "metadata": {
        "id": "PO3s3QUr-tcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 간단한 문제를 계산 그래프를 사용해서 풀어보자."
      ],
      "metadata": {
        "id": "8FSv3BzG-v01"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__문제1__  \n",
        "현빈이가 슈퍼에서서 1개에 100원인 사과를 2개 샀다.  \n",
        "이때 지불 금액을 구하여라. 단, 소비세가 10% 부과된다."
      ],
      "metadata": {
        "id": "26djfVVU--Q6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 계산 그래프는 계산 과정을 노드와 화살표로 표현한다.\n",
        "+ 노드는 원으로 표기하고 원 안에 연산 내용을 적는다."
      ],
      "metadata": {
        "id": "d7JSiszo_Fv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1X0DjjN6XFBNmQ-Zt9PnjGfmnAO1HXVfJ' width = 550 /><br>"
      ],
      "metadata": {
        "id": "mKe5Zezb_fuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ ×2와 ×1.1 을 각각 하나의 연산으로 취급하지말고 곱셈인 ×만 연산으로 생각해보자.\n",
        "+ '2'와 '1.1'은 각각 '사과의 개수'와 '소비세' 변수가 되어 원 밖에 표시된다."
      ],
      "metadata": {
        "id": "o2hylnJZ_qO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1isLxoyt3mJxaUuU4fUQnEWwdNrlQIne1' width = 550><br>"
      ],
      "metadata": {
        "id": "kkNgwOvbBx45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__문제2__  \n",
        "현빈이가 슈퍼에서 사과 2개, 귤 3개를 샀다.  \n",
        "사과는 1개에 100원, 귤 1개에 150원이다. 소비사 10%일 때 지불 금액을 구하시오."
      ],
      "metadata": {
        "id": "DoLgL4p6CD_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 문제2도 계산 그래프로 풀어보자."
      ],
      "metadata": {
        "id": "hW-5-IP-CjTY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1g_2xEcL5Mf6DVAYDubqwrFLjb9a26VQc' width = 550 /><br>"
      ],
      "metadata": {
        "id": "CdiPY-vCCz6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 이 문제에는 덧셈 노드가 새로 등장하여 사과와 귤의 금액을 합산한다.\n",
        "+ 왼쪽에서 오른쪽으로 계산을 진행하면 답은 715원이다."
      ],
      "metadata": {
        "id": "PiuNG9TmDDqg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 계산 그래프를 이용한 문제풀이는 다음 흐름으로 진행한다."
      ],
      "metadata": {
        "id": "cAOxPeJzDQM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 계산 그래프를 구성한다.\n",
        "2. 그래프에서 계산을 왼쪽에서 오른쪽으로 진행한다."
      ],
      "metadata": {
        "id": "KsDvND-tDUJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 여기서 2번째 '계산을 왼쪽에서 오른쪽으로 진행' 하는 단계를 __순전파__라고 한다.\n",
        "+ 순전파의 반대 방향의 전파도 가능한데 그것을 __역전파__ 라고 한다.\n",
        "+ 역전파는 이후에 미분을 계산할 때 중요한 역할을 한다."
      ],
      "metadata": {
        "id": "gx5UpKmUDYLZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1.2 국소적 계산"
      ],
      "metadata": {
        "id": "Cd2kLQj1FOYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 계산 그래프는 국소적 계산을 전파함으로써 최종 결과를 얻는 특징이 있다.\n",
        "+ 국소적이란 '자신과 직접 관계된 작은 범위' 라는 뜻이다.\n",
        "+ 전체에서 어떤 일이 벌어지든 상관없이 자신과 관계된 정보만으로 결과를 출력할 수 있다.\n",
        "+ 국소적 계산의 예를 살펴보자."
      ],
      "metadata": {
        "id": "QZPE8co2Sjet"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1PzbkfRY2iA_K9VBdjzwaLSNxWyxNbAOT' width = 550><br>"
      ],
      "metadata": {
        "id": "pnXAUE0TVRS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 위 예에서는 여러 식품을 구입하여 (복잡한 계산을 거쳐) 총 금액이 40,000원이 되었다.\n",
        "+ 핵심은 각 노드에서의 계산은 국소적 계산이라는 점이다.\n",
        "+ 사과와 그 외의 물품 값을 더하는 계산(4,000+200 → 4,200)은  \n",
        "4,000 이라는 숫자가 어떻게 계산되었는지 상관없이 단지 두 숫자를 더하면 된다는 뜻이다.\n",
        "+ 각 노드는 자신과 관련한 계산 외에는 아무것도 신경 쓸 게 없다.\n",
        "+ 국소적인 계산은 단순하지만 그 결과를 전달함으로써 전체를 구성하는 복잡한 계산을 할 수 있다.\n"
      ],
      "metadata": {
        "id": "_ucaoUUnVoKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1.3 왜 계산 그래프로 푸는가?"
      ],
      "metadata": {
        "id": "ENDGTDZHWVyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "계산 그래프의 이점"
      ],
      "metadata": {
        "id": "Ye_b2iJXB8Fc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 전체가 복잡해도 각 노드에서는 단순한 계산에 집중하여 문제를 단순화할 수 있다.\n",
        "+ 중간 계산 결과를 모두 보관할 수 있다.\n",
        "+ 역전파를 통해 '미분'을 효율적으로 계산할 수 있다."
      ],
      "metadata": {
        "id": "bajpXFp7B90Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "계산 그래프에서의 역전파"
      ],
      "metadata": {
        "id": "nhcL7NmbUeFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 문제 1은 사과를 2개사서 소비세를 포함한 최종 금액을 구하는 것이다.\n",
        "+ 사과 가격이 오르면 최종 금액에 어떤 영향을 끼치는지 알고 싶다고 하자.  \n",
        " 이는 사과 가격에 대한 지불 금액의 미분을 구하는 문제이다.\n",
        "+ 사과 값을 $x$, 지불 금액을 $L$이라 했을 때 $ \\frac{∂L}{∂x}$ 을 구하는 것이다.\n",
        "+ 아래 그림처럼 계산 그래프 상의 역전파에 의해 미분을 구할 수 있다."
      ],
      "metadata": {
        "id": "ZjJhtI8iCDHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1ulSa6QvmojR2snm1HVgWMqBykjkJ1kVl' width = 550><br>"
      ],
      "metadata": {
        "id": "taejIiygVx_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 역전파는 순전파와는 반대 방향의 화살표(굵은 선)로 그린다.\n",
        "+  이 전파는 '국소적 미분'을 전달하고 미분 값은 화살표 아래에 적는다.\n",
        "+ 이 예에서 역전파는 오른쪽에서 왼쪽으로 1→1.1→2.2 순으로 미분 값을 전달한다."
      ],
      "metadata": {
        "id": "aqpadqcHXA2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 위 예에서는 사과 가격에 대한 미분만 구했지만 다른 변수에 대한 미분도 같은 순서로 구할 수 있다.\n",
        "+ 또한 중간까지 구한 미분 결과를 공유할 수 있어서 다수의 미분을 효율적으로 계산할 수 있다.\n",
        "+ 계산 그래프는 순전파와 역전파를 활용하여 각 변수의 미분을 효율적으로 구할 수 있다."
      ],
      "metadata": {
        "id": "wqnEhfL3YBxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 연쇄법칙"
      ],
      "metadata": {
        "id": "unWkBbv2Yc7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 순전파는 계산 결과를 왼쪽에서 오른쪽으로 전달하지만  \n",
        "역전파는 국소적인 미분을 오른쪽에서 왼쪽으로 전달한다.\n",
        "+ 역전파의 원리는 __연쇄법칙__에 따른 것이다.\n",
        "+ 연쇄법칙이 계산 그래프 상의 역전파와 같다는 것을 보이자."
      ],
      "metadata": {
        "id": "7IcpFi1iYeH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2.1 계산 그래프의 역전파"
      ],
      "metadata": {
        "id": "MO4nPm8_Z8rw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ $ y = f(x) $ 라는 계산의 역전파를 계산 그래프로 그려보자."
      ],
      "metadata": {
        "id": "3wlGrXqVwYzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=18OCPoVccKOTVKF5hn51OCIYFCXfEUm9l' width = 550/><br>"
      ],
      "metadata": {
        "id": "_L8AVBH0yC6q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 역전파의 계산 절차는 신호 $E$에 노드의 국소적 미분( $\\frac{∂y}{∂x}$)을 곱한 후 다음 노드로 전달하는 것이다.\n",
        "+ 국소적 미분이란 순전파 때의 $ y = f(x)$ 계산의 미분을 구한다는 것이며,  \n",
        "이는 x에 대한 y의 미분 ( $\\frac{∂y}{∂x}$)을 구한다는 뜻이다.\n",
        "+ 이 국소적인 미분을 상류에서 전달된 값에 곱해 앞쪽 노드로 전달하는 것이다."
      ],
      "metadata": {
        "id": "6oi6iY5s0bq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2.2 연쇄법칙이란?"
      ],
      "metadata": {
        "id": "OP5xwKlb1OO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 연쇄법칙은 합성 함수의 미분에 대한 성질이다.\n",
        "+ 합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.\n",
        "+ 수식으로는 다음과 같다."
      ],
      "metadata": {
        "id": "CYLUUYQH1shg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ \\Large \\frac{∂z}{∂x} = \\frac{∂z}{∂t}\\frac{∂t}{∂x}$"
      ],
      "metadata": {
        "id": "VL5FqO172X_e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 간단한 예를 살펴보자."
      ],
      "metadata": {
        "id": "4h3Cm2487b9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ z = t^2 \\\\ t = x + y$"
      ],
      "metadata": {
        "id": "ZDOr_Oss7Q3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "연쇄법칙을 써서 미분 $ \\frac{∂z}{∂x}$을 구해보자."
      ],
      "metadata": {
        "id": "qHyAvpnm7a6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\Large \\frac{∂z}{∂t} = 2t ,  \\frac{∂t}{∂x} = 1$ <br/><br/>\n",
        "$\\Large \\frac{∂z}{∂x} = \\frac{∂z}{∂t}\\frac{∂t}{∂x} = 2t·1 = 2(x+y) $"
      ],
      "metadata": {
        "id": "8XE-nO977j82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2.3 연쇄법칙과 계산 그래프"
      ],
      "metadata": {
        "id": "mRHxjKI_8fEj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 연쇄법칙 계산을 계산 그래프로 나태내보자."
      ],
      "metadata": {
        "id": "6WJSxysIs6Dy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1V9r6z4Ws46rpgJ4SHbLzguigm_afu9Qu' width = 550><br>"
      ],
      "metadata": {
        "id": "ZUsnfSRYs8fP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 노드로 들어온 입력 신호에 그 노드의 국소적 미분(편미분)을 곱한 후 다음 노드로 전달한다.\n",
        "+ 위 예를 보면 입력은 $\\frac{∂z}{∂z}$ 이고 편미분인 $ \\frac{∂z}{∂t}$을 곱하고 다음 노드로 넘긴다.\n",
        "+ 맨 왼쪽 역전파를 주목하자.  \n",
        "$ \\frac{∂z}{∂z}\\frac{∂z}{∂t}\\frac{∂t}{∂x} =\\frac{∂z}{∂t}\\frac{∂t}{∂x} = \\frac{∂z}{∂x} $ 가 성립되어 'x에 대한 z의 미분'이 된다. <br/>\n",
        "\n",
        "+ 즉 역전파가 하는 일은 연쇄법칙의 원리와 같다."
      ],
      "metadata": {
        "id": "J2g31RLItwp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 역전파"
      ],
      "metadata": {
        "id": "0fL7Lh5oyLbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ '+'와 '×' 등의 연산을 예로 역전파의 구조를 설명한다."
      ],
      "metadata": {
        "id": "llhsTYWQybN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3.1 덧셈 노드의 역전파"
      ],
      "metadata": {
        "id": "F-y4cBP0zJCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ z = x + y $  을 대상으로 역전파를 살펴보자. 우선 미분은 다음과 같이 계산된다.\n"
      ],
      "metadata": {
        "id": "JAgPo8Cw0H3a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ \\Large \\frac{∂z}{∂x} = 1 $  \n",
        "\n",
        "$ \\Large \\frac{∂z}{∂y} = 1 $"
      ],
      "metadata": {
        "id": "Zb20hh6G9NwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이를 계산 그래프로 다음과 같이 그릴 수 있다."
      ],
      "metadata": {
        "id": "pQgwgvYrBhgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1H5hH4IsMO6jS24MJu8FI9PNUSaIZX5LL' width = 550><br>"
      ],
      "metadata": {
        "id": "XftqvnFMB3w8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 덧셈 노드의 역전파는 1을 곱하기만 할 뿐 입력된 값을 그대로 다음 노드로 보낸다.  \n",
        "+ 이 예에서는 상류에서 전해진 미분 값을  $\\frac{∂L}{∂z}$이라 했는데,  \n",
        "이는 최종적으로 L 값을 출력하는 큰 계산그래프를 가정하기 때문이다.\n",
        "+ $ z = x+y$ 계산은 상류로부터 $\\frac{∂L}{∂z}$ 값이 전해지고, 하류로는 $\\frac{∂L}{∂x}$과 $\\frac{∂L}{∂y}$ 값을 전달한다."
      ],
      "metadata": {
        "id": "f3rLX9LfB6ut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1vGm0-UIkLoVWzWRSsCcyWXmfoU_Uogcw' width = 550 /><br>"
      ],
      "metadata": {
        "id": "SgN3MpTzdRnm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 구체적인 예를 살펴보면, '$10 + 5 = 15$' 라는 계산이 있고, 상류에서 1.3 값이 흘러온다. 계산 그래프로 그리면 다음과 같다.\n",
        "+ 덧셈 노드 역전파는 입력 신호를 다음 노트로 출력할 뿐이고, 1.3을 그대로 다음 노드로 전달한다."
      ],
      "metadata": {
        "id": "bmUsQBxdd1LQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1ZepeHZn_pPjkXrBa9eR3Sv6P0yNMdJLN' width = 550><br>"
      ],
      "metadata": {
        "id": "ka_qpOv5e623"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3.2 곱셈 노드의 역전파"
      ],
      "metadata": {
        "id": "-lXKatg6e-yJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ $z = xy$ 라는 식을 보자. 이 식의 미분은 다음과 같다."
      ],
      "metadata": {
        "id": "15HlPsExfFXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ \\Large \\frac{∂z}{∂x} = y $  \n",
        "\n",
        "$ \\Large \\frac{∂z}{∂y} = x $"
      ],
      "metadata": {
        "id": "a2-tsZCLfMQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "계산 그래프는 다음과 같이 그릴 수 있다."
      ],
      "metadata": {
        "id": "_nSH-zDsfSIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1go7BW4Y4YBJrYXNaGGYyIrxMk0hjLlF3' width = 550><br>"
      ],
      "metadata": {
        "id": "XV8T8yZhfU92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 곱셈 노드 역전파는 상류의 값에 순전파 때의 입력 신호들을 '서로 바꾼 값'을 곱해서 하류로 보낸다.\n",
        "+ 서로 바꾼 값이란 위 그림처럼 순전파 때의 $x$ 였다면 역전파에서는 $y$,  \n",
        "순전파 때 $y$ 였다면 역전파에서는 $x$로 바꾼다는 의미이다."
      ],
      "metadata": {
        "id": "dGidVjrafpab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 구체적인 예를 살펴보면, '$10 × 5 = 50$' 이라는 계산이 있고, 역전파 때 상류에서 1.3 값이 흘러온다고 하자.\n",
        "+ 이를 계산 그래프로 그리면 다음과 같다."
      ],
      "metadata": {
        "id": "blS1nakfgBCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=127IneGmdQUlQKGfU7b6dgkvAhwVmIz25' width = 550 /><br>"
      ],
      "metadata": {
        "id": "Sd30xCp_gWv1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 입력 신호를 바꾼 값을 곱하여 하나는 $1.3 × 5 = 6.5$ , 다른 하나는 $1.3 × 10 = 13$ 이 된다.\n",
        "+ 덧셈의 역전파와 달리, 곱셈의 역전파는 순방향 입력 신호의 값이 필요하다.  \n",
        "그래서 곱셈 노드를 구현할 때는 순전파의 입력 신호를 변수에 저장해둔다."
      ],
      "metadata": {
        "id": "uWr30ZT6gb0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3.3 사과 쇼핑의 예"
      ],
      "metadata": {
        "id": "DxLespHkiUvD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 사과 쇼핑 예를 다시 살펴보자.\n",
        "+ 사과의 가격, 사과의 개수, 소비세라는 세 변수 각각이 최종 금액에 어떻게 영향을 주느냐를 풀고자 한다.\n",
        "+ '사과 가격에 대한 지불 금액의 미분', '사과 개수에 대한 지불 금액의 미분',  \n",
        "'소비세에 대한 지불 금액의 미분'을 구하는 것이 해당된다.\n",
        "+ 이를 계산 그래프의 역전파를 사용해서 풀면 다음과 같다."
      ],
      "metadata": {
        "id": "25u6gIWCiafm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=16sDIRSasisHs6SaxK97vN9RPF1rCsc9A' width = 550/><br>"
      ],
      "metadata": {
        "id": "b4ZH1nEMjEfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 사과 가격의 미분은 2.2, 사과 개수의 미분은 110, 소비세의 미분은 200이다.\n",
        "+ 이는 소비세와 사과 가격이 같은 양만큼 오르면 최종 금액에는 소비세가 200의 크기로,  \n",
        "사과 가격이 2.2 크기로 영향을 준다고 해석할 수 있다.\n",
        "+ 단, 이 예에서 소비세와 사과 가격은 단위가 다르니 주의해야 한다.(소비세 1은 100%, 사과 가격 1은 1원).\n",
        "+ 마지막으로 '사과와 귤 쇼핑'의 역전파를 풀어보자. 빈칸에 각 변수의 미분을 구해서 채워보자.  \n",
        "(정답은 뒤에 나옴)"
      ],
      "metadata": {
        "id": "WuivjaEqjNLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1gdjwvahGj0aYmto0yVc5IOQFnp9FC3wL' width = 550 /><br>"
      ],
      "metadata": {
        "id": "t_1SrS7hj-QH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4 단순한 계층 구현하기"
      ],
      "metadata": {
        "id": "y2d4A03Npq3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 앞서 보아온 '사과 쇼핑' 예를 파이썬으로 구현해보자.\n",
        "+ 계산 그래프의 곱셈 노드를 'MulLayer', 덧셈 노드를 'AddLayer'라는 이름으로 구현한다."
      ],
      "metadata": {
        "id": "sNiHb582vUDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4.1 곱셈 계층"
      ],
      "metadata": {
        "id": "sfXh9T24viMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 모든 계층은 forward( )와 backward( )라는 공통의 매서드를 갖도록 구현한다.\n",
        "+ forward( )는 순전파, backward( )는 역전파를 처리한다."
      ],
      "metadata": {
        "id": "f-fQUDT2vkdx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 곱셈 계층은 다음과 같이 구현한다."
      ],
      "metadata": {
        "id": "oSNCEPifvwqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MulLayer:\n",
        "  def __init__(self):\n",
        "    self.x = None\n",
        "    self.y = None\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    out = x * y\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = dout * self.y # x와 y를 바꾼다.\n",
        "    dy = dout * self.x\n",
        "\n",
        "    return dx, dy"
      ],
      "metadata": {
        "id": "pGJqEpKo1Qdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ _ _ init _ _( )에서 인스턴스 변수인 x, y를 초기화한다. 두 변수는 순전파 시의 입력 값을 유지하기 위해 사용한다.\n",
        "+ forward( )에서는 x와 y를 인수로 받고 두 값을 곱해서 반환한다.\n",
        "+ backward( )에서는 상류에서 넘어온 미분(dout)에 순전파 때의 값을 '서로 바꿔' 곱한 후 하류로 흘린다."
      ],
      "metadata": {
        "id": "f-6iY8_101fM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 이 MulLayer을 사용해서 앞서 본 '사과 쇼핑'을 구현해보자."
      ],
      "metadata": {
        "id": "G6wRumLM1VAB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1Jrbk5vQIcNDWQdmIaS_ehH5FEIDup9ng' width = 550 /><br>"
      ],
      "metadata": {
        "id": "LvPjuEn-1t3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "apple = 100\n",
        "apple_num = 2\n",
        "tax = 1.1\n",
        "\n",
        "# 계층들\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "# 순전파\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
        "price = mul_tax_layer.forward(apple_price, tax)\n",
        "\n",
        "print(price)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiKFVgcmwLI9",
        "outputId": "b2c5a6c2-667a-4673-ad15-ef41cd35e85b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "220.00000000000003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 각 변수에 대한 미분은 backward( )에서 구할 수 있다."
      ],
      "metadata": {
        "id": "SG6y-zbi2Rvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 역전파\n",
        "dprice = 1\n",
        "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
        "\n",
        "print(dapple, dapple_num, dtax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5PiABnA2Ktw",
        "outputId": "d7af85bd-8902-4b04-e9be-f9c345e868fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2 110.00000000000001 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ backward( ) 호출 순서는 forward( )때와는 반대이고, backward( )가 받는 인수는 '순전파의 출력에 대한 미분'이다.\n",
        "+ mul_apple_layer 라는 곱셈 계층은 순전파 때는 apple_price를 출력하지만,  \n",
        "역전파 때는 apple_price 의 미분 값인 dapple_price를 인수로 받는다."
      ],
      "metadata": {
        "id": "zwgrPLCO2mgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4.2 덧셈 계층"
      ],
      "metadata": {
        "id": "-kZnBv2J2-5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 덧셈 계층을 구현해보자."
      ],
      "metadata": {
        "id": "HHEtk2SS3A1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AddLayer:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    out = x + y\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = dout * 1\n",
        "    dy = dout * 1\n",
        "    return dx, dy"
      ],
      "metadata": {
        "id": "yobijNgl2w3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 덧셈 계층에서는 초기화가 필요없으니 _ _ init _ _( )에서는 아무 일도 하지 않는다.\n",
        "+ forward( )에서는 입력받은 두 인수 x, y를 더해서 반환한다.\n",
        "+ backward( )에서는 상류에서 내려온 미분(dout)을 그대로 하류로 흘린다."
      ],
      "metadata": {
        "id": "TUdeZoVe3U0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 덧셈 계층과 곱셈 계층을 사용하여 사과 2개와 귤 3개를 사는 상황을 구현해보자."
      ],
      "metadata": {
        "id": "qAGSkBsy3mED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1HDDb9fyPvCF0vEQSHMu7KOzei2pXfBAe' width = 550/><br>"
      ],
      "metadata": {
        "id": "4Rpi0Ye83qE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "apple = 100\n",
        "orange = 150\n",
        "apple_num = 2\n",
        "orange_num = 3\n",
        "tax = 1.1\n",
        "\n",
        "# 계층들\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_orange_layer = MulLayer()\n",
        "add_apple_orange_layer = AddLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "\n",
        "\n",
        "# 순전파\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
        "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
        "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
        "price = mul_tax_layer.forward(all_price, tax)\n",
        "\n",
        "# 역전파\n",
        "dprice = 1\n",
        "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
        "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
        "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
        "dapple, apple_num = mul_apple_layer.backward(dapple_price)\n",
        "\n",
        "print(price)\n",
        "print(dapple, dapple_num, dorange, dorange_num, dtax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEzXJw083T-c",
        "outputId": "3f3fda14-5e85-4323-c049-0bbb786d3ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "715.0000000000001\n",
            "2.2 110.00000000000001 3.3000000000000003 165.0 650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 필요한 계층을 만들어 순전파 메서드인 forward( )를 적절한 순서로 호출한다.\n",
        "+ 순전파와 반대 순서로 역전파 메서드인 backward( )를 호출하면 원하는 미분이 나온다."
      ],
      "metadata": {
        "id": "u1LTWoLKrySp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.5 활성화 함수 계층 구현하기"
      ],
      "metadata": {
        "id": "EQs2G79Ir-DG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 계산 그래프를 신경망에 적용해보자.\n",
        "+ 신경망을 구성하는 층 각각을 클래스 하나로 구현한다.\n",
        "+ 활성화 함수인 ReLU와 Sigmoid 계층을 구현한다."
      ],
      "metadata": {
        "id": "E4h8xSFriim9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.5.1 ReLU 계층"
      ],
      "metadata": {
        "id": "PQaV6qQMiht5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 활성화 함수로 사용되는 ReLU의 수식은 다음과 같다."
      ],
      "metadata": {
        "id": "J_JPPloai0W0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$y =\n",
        "\\begin{cases}\n",
        "x \\;(x > 0)\\\\\n",
        "0 \\;(x \\leq 0)\n",
        "\\end{cases}$"
      ],
      "metadata": {
        "id": "H_WZozHbi31a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ x에 대한 y의 미분은 다음과 같다."
      ],
      "metadata": {
        "id": "Hofay4KAm9sw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ \\frac{∂y}{∂x} =\n",
        "\\begin{cases}\n",
        "1 \\;(x > 0)\\\\\n",
        "0 \\;(x \\leq 0)\n",
        "\\end{cases}$"
      ],
      "metadata": {
        "id": "cgUbPGbEnDIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 순전파 때의 입력인 $x$가 0보다 크면 역전파는 상류의 값을 그대로 하류로 흘린다.\n",
        "+ 순전파 때 $x$가 0 이하면 역전파 때는 하류로 신호를 보내지 않는다.\n",
        "+ 계산 그래프로 표현은 다음과 같다."
      ],
      "metadata": {
        "id": "TEUoWG-Tnl2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1sj0LrwDwrX2oZ4CyODUpUEe8Y-CNkAbS' width = 550/><br>"
      ],
      "metadata": {
        "id": "gBXCTTVToKii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 신경망 계층의 forward( )와 backward( ) 함수는 넘파이 배열을 인수로 받는다고 가정하고 ReLU 계층을 구현해보자."
      ],
      "metadata": {
        "id": "ifI8-wuNoR4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Relu:\n",
        "  def __init__(self):\n",
        "    self.mask = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.mask = (x <= 0)\n",
        "    out = x.copy()\n",
        "    out[self.mask] = 0\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dout[self.mask] = 0\n",
        "    dx = dout\n",
        "\n",
        "    return dx"
      ],
      "metadata": {
        "id": "ovxnlkZIqnjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Relu 클래스는 mask라는 인스턴스 변수를 가진다.\n",
        "+ mask는 True/False로 구성된 넘퍼아 배열로,  \n",
        "순전파의 입력인 x의 원소 값이 0 이하인 인덱스는 True, 그 외는 False로 유지한다."
      ],
      "metadata": {
        "id": "upwsESfOqNjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x = np.array([[1.0, -0.5], [-2.0, 3.0]] )\n",
        "print(x)\n",
        "\n",
        "mask = (x<=0)\n",
        "print(mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv6yxgTUoyhs",
        "outputId": "73a4651a-c757-4739-9bfc-097dad1499c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.  -0.5]\n",
            " [-2.   3. ]]\n",
            "[[False  True]\n",
            " [ True False]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 순전파 때의 입력 값이 0 이하면 역전파 떄의 값은 0이 돼야 한다.\n",
        "+ 그래서 역전파 때는 순전파 때 만들어둔 mask를 써서 mask 원소가 True 인 곳에는 상류에서 전파된 dout을 0으로 설정한다."
      ],
      "metadata": {
        "id": "6d9ccfmiCOlh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.5.2 Sigmoid 계층"
      ],
      "metadata": {
        "id": "d1kKSg8AC2xn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 시그모이드 함수는 다음 식을 의미하는 함수이다."
      ],
      "metadata": {
        "id": "PpTSIFjDC5vb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ \\Large y = \\frac{1}{1+ e^{-x}}$"
      ],
      "metadata": {
        "id": "Ov68fd9rC9KF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 계산 그래프로 그리면 다음과 같다."
      ],
      "metadata": {
        "id": "TSoO1g2BDQaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1Xh6KGjX6ncsV2A_CV15dTC6qNBPrToAV' width = 550/><br>"
      ],
      "metadata": {
        "id": "wF8JFFjgDSZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 'exp'와 '/' 노드가 새롭게 등장했다.  \n",
        "'exp'노드는 y = exp(x) 계산을 수행하고 '/' 노드는 y = $\\frac{1}{x}$ 계산을 수행한다.\n",
        "+ 위 식의 계산은 국소적 계산의 전파로 이뤄진다.\n",
        "+ 위 식의 역전파를 알아보자."
      ],
      "metadata": {
        "id": "leRWzzGOEnSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__1단계__\n",
        "+ $ y = \\frac{1}{x}$을 미분하면 다음 식이 된다.  "
      ],
      "metadata": {
        "id": "gQ_1Wz4lFLr-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ $ \\Large \\frac{\\partial y}{∂x} = -\\frac{1}{x^{2}} = - y^{2}$"
      ],
      "metadata": {
        "id": "KDAUihUjFr9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 역전파 때는 상류에서 흘러온 값에 $-y^{2}$ 을 곱해서 하류로 전달한다.\n",
        "+ 계산 그래프에서는 다음과 같다."
      ],
      "metadata": {
        "id": "TTKUJ9gnFsoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1A3CbZ9cngWh0rqY58qHlhOzWfZ9H6UgP' width = 550/><br>"
      ],
      "metadata": {
        "id": "dlBwnI2uGEsH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__2단계__\n",
        "+ '+' 노드는 상류의 값을 여과 없이 하류로 내보낸다. 계산 그래프는 다음과 같다."
      ],
      "metadata": {
        "id": "qAZyL7ilI2dp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1ojRDt_ueWc6VoWMX8lOVnoUdjdt0Ik6l' width = 550/><br>"
      ],
      "metadata": {
        "id": "KK-Fg6EZJAhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__3단계__\n",
        "+ 'exp' 노드는 $y = e^{x}$ 연산을 수행하며, 미분은 다음과 같다."
      ],
      "metadata": {
        "id": "OQjUM5qmJL9p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+  $ \\Large \\frac{\\partial y}{∂x} = e^{x}$\n",
        "\n",
        "+ 계산 그래프에서는 상류의 값에 순전파 때의 출력을 곱해 하류로 전파한다."
      ],
      "metadata": {
        "id": "Z1xyvw_sJiGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1Bq73iuvMFYxAugSL9cuB44uZ0lu5iToQ' width = 550/><br>"
      ],
      "metadata": {
        "id": "JhHJuYXDJZ7Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__4단계__\n",
        "+ '×' 노드는 순전파 때의 값을 '서로 바꿔' 곱한다. 이 예에서는 -1을 곱하면 된다."
      ],
      "metadata": {
        "id": "V5UeAHCiJyQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1ksAFP0GEUDMfJTLfFMtyy4ytOfipQkNs' width = 550 /><br>"
      ],
      "metadata": {
        "id": "Dikfqeh8KOtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 역전파의 최종 출력인 $\\frac{∂L}{∂y} y^{2}e^{-x}$ 값이 하류 노드로 전파된다.\n",
        "+ 순전파의 입력 $x$와 출력 $y$만으로  $\\frac{∂L}{∂y} y^{2}e^{-x}$ 를 계산할 수 있다.\n",
        "+ 계산 그래프의 중간 과정을 모두 묶어서 아래처럼 단순한 'sigmoid' 노드 하나로 대체할 수 있다."
      ],
      "metadata": {
        "id": "MCY897biNeeH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1xpbcWlL4LN_wxln-oh4TwXZ0HANUH_Oi' width = 550 /><br>"
      ],
      "metadata": {
        "id": "7rLW3MVRM3j9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 간소화 버전은 역전파 과정의 중간 계산들을 생략할 수 있어 효율적으로 계산할 수 있다.\n",
        "+ 노드를 그룹화하여 Sigmoid 계층의 세세한 내용을 노출하지 않고 입력과 출력에만 집중 할 수 있다."
      ],
      "metadata": {
        "id": "CJh48KY-TIhJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "또한  $\\frac{∂L}{∂y} y^{2}e^{-x}$ 는 아래처럼 정리해서 쓸 수 있다."
      ],
      "metadata": {
        "id": "pEdrLpkHTWuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1b00UUMUUl4n5yJDi0pBZpqRPMaVhiel6' width = 550/><br>"
      ],
      "metadata": {
        "id": "cA7afaEqTon8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 이처럼 Sigmoid 계층의 역전파는 순전파의 출력(y)만으로 계산할 수 있다."
      ],
      "metadata": {
        "id": "rswjUTSQTzcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1Cjimaz7lKU0sbGYsZbP8Ury52gnn0Byd' width = 550 /><br>"
      ],
      "metadata": {
        "id": "6LAk1ChfUDk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ sigmoid 계층을 파이썬으로 구현해보자."
      ],
      "metadata": {
        "id": "v1ZBcFavXZVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid:\n",
        "  def __init__(self):\n",
        "    self.out = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = 1 / (1 + np.exp(-x))\n",
        "    self.out = out\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "    return dx"
      ],
      "metadata": {
        "id": "eoRqKTxmquPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 순전파의 출력을 인스턴스 변수 out에 보관했다가, 역전파 계산 때 그 값을 사용한다."
      ],
      "metadata": {
        "id": "KbDfWQ6bXvy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.6 Affine / Softmax 계층 구현하기"
      ],
      "metadata": {
        "id": "c7_UJWOTX4NB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.6.1 Affine 계층"
      ],
      "metadata": {
        "id": "XJonpQTzYBbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 신경망의 순전파에서는 가중치 신호의 총합을 계산하기 때문에 행렬의 곱을 사용했다."
      ],
      "metadata": {
        "id": "yR47KUvfYDmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.random.rand(2) # 입력\n",
        "W = np.random.rand(2,3) # 가중치\n",
        "B = np.random.rand(3) # 편향\n",
        "Y = np.dot(X, W) + B\n",
        "\n",
        "print(X.shape)\n",
        "print(W.shape)\n",
        "print(B.shape)\n",
        "print(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_24x0cNXvi0",
        "outputId": "23297ceb-be8e-46d6-fc95-d13c493059a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2,)\n",
            "(2, 3)\n",
            "(3,)\n",
            "[0.36677571 0.5352985  0.61640711]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 여기에서 X, W, B는 각각 형상이 (2,), (2,3), (3,)인 다차원 배열이다.\n",
        "+ 뉴런의 가중치 합은 Y = np.dot(X, W) + B로 계산한다.\n",
        "+ Y를 활성화 함수로 변환해 다음 층으로 전파하는 것이 신경망 순전파의 흐름이다."
      ],
      "metadata": {
        "id": "Y6Hs75mxYmJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1I5SvD_Yslv2bhnNxj0IGEHFAbf_SZsfX' width = 550 /><br>"
      ],
      "metadata": {
        "id": "oHucxKvpY--R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__NOTE__  \n",
        "신경망의 순전파 때 수행하는 행렬곱은 기하학에서 __어파인 변환__이라고 한다.  \n",
        "여기서는 어파인 변환을 수행하는 처리를 'Affine 계층'이라는 이름으로 구현한다."
      ],
      "metadata": {
        "id": "JVuRP5u-ZMDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 앞에서 수행한 계산을 계산 그래프로 그려보자."
      ],
      "metadata": {
        "id": "PCxOWnnfZWep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1_qubCauhlxi3Z08PO1-8AWdtx8zJ8Qic' width = 550 /><br>"
      ],
      "metadata": {
        "id": "wIsIDlxnZs3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 곱을 계산하는 노드를 'dot'라고 하면 np.dot(X, W) + B 계산은 위 그림처럼 그려진다.\n",
        "+ 각 변수의 이름 위에 그 변수의 형상도 표기한다.\n",
        "+ 지금까지의 계산 그래프는 노드 사이에 '스칼라 값'이 흘렀는데 이 예는 '행렬'이 흐르는 것이다."
      ],
      "metadata": {
        "id": "7H4LGHNHZuri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 위 그림의 역전파에 대해 생각해보자.\n",
        "+ 행렬을 사용한 역전파도 행렬의 원소마다 전개해보면  \n",
        "스칼라값을 사용한 계산 그래프와 같은 순서로 생각할 수 있다.\n",
        "+ 전개한 식은 다음과 같다."
      ],
      "metadata": {
        "id": "WnNnG6x8aAIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ \\Large \\frac{∂L}{∂X} = \\frac{∂L}{∂Y}·W^{T}$ <br/><br/>\n",
        "$ \\Large \\frac{∂L}{∂W} = X^{T}·\\frac{∂L}{∂Y}$"
      ],
      "metadata": {
        "id": "CGSI7PymaNoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ $W^{T}$의 T는 전치행렬을 뜻한다.\n",
        "+ 위 식을 바탕으로 계산 그래프의 역전파를 구해보자."
      ],
      "metadata": {
        "id": "lPKtbje5a8GY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=12DIyALPnEne5OqqWCdpeOkqd_ja9X2kz' width = 550/><br>"
      ],
      "metadata": {
        "id": "FkgAJ6SsbXpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 위 계산 그래프에서 각 변수의 형상에 주의해서 보자.\n",
        "+ $X$와 $\\frac{∂L}{∂X}$는 같은 형상이고, $W$와 $\\frac{∂L}{∂W}$도 같은 형상이다."
      ],
      "metadata": {
        "id": "aLicZrTScGLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large X = (x_0, x_1, ···, x_n)$ <br/><br/>\n",
        "$\\large \\frac{∂L}{∂X} = (\\frac{∂L}{∂x_0}, \\frac{∂L}{∂x_1}, ···, \\frac{∂L}{∂x_2})$"
      ],
      "metadata": {
        "id": "pIW8S2TscqLD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 행렬의 형상에 주의해야 하는 이유는  \n",
        "행렬의 곱에서 대응하는 차원의 원소 수를 일치시켜야 하기 때문이다."
      ],
      "metadata": {
        "id": "CnG7oRYVdH5X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1W809Wwt5RNKUYPXAD0x2ieTrYAl2xC3I' width = 550/><br>"
      ],
      "metadata": {
        "id": "9_JO_it1dvEh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.6.2 배치용 Affine 계층"
      ],
      "metadata": {
        "id": "FFxKT9jDeL6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 지금까지 Affine 계층은 입력 데이터로 $X$ 하나만을 고려한 것이다.\n",
        "+ 데이터 N개를 묶어 순전파하는 경우, 즉 배치용 Affine 계층을 생각해보자.\n",
        "+ 배치용 Affine 계층을 게산 그래프로 그려보자."
      ],
      "metadata": {
        "id": "ccfmNbl6eOZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=16J9JgubXtdxj7Lb5VbjXQTuJiWcZZnOU' width =550/><br>"
      ],
      "metadata": {
        "id": "ye5bCTqaen0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 기존과 다른 부분은 입력인 $X$의 형상이 (N, 2)가 된 것뿐이다.\n",
        "+ 지금까지와 같이 계산 그래프의 순서를 따라 행렬 계산을 하게 된다.\n",
        "+ 역전파 때는 행렬의 형상에 주의하면 $\\frac{∂L}{∂X}$과 $\\frac{∂L}{∂W}$은 이전과 같이 도출할 수 있다.\n",
        "+ 순전파 때의 편향 덧셈은 $X·W$에 대한 편향이 각 데이터에 더해진다. 구체적인 예를 살펴보자."
      ],
      "metadata": {
        "id": "t4O3PWMOhSMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_dot_W = np.array([[0,0,0],[10,10,10]])\n",
        "B = np.array([1,2,3])\n",
        "\n",
        "print(X_dot_W)\n",
        "print(X_dot_W + B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVT-4_RQYanj",
        "outputId": "34355f98-e63d-4cef-c130-e7c6fcc18f5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  0]\n",
            " [10 10 10]]\n",
            "[[ 1  2  3]\n",
            " [11 12 13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 순전파의 편향 덧셈은 각각의 데이터에 더해진다.\n",
        "+ 역전파 때는 각 데이터의 역전파 값이 편향의 원소에 모여야 한다.\n"
      ],
      "metadata": {
        "id": "jQHLn7O5kign"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dY = np.array([[1,2,3],[4,5,6]])\n",
        "print(dY)\n",
        "\n",
        "dB = np.sum(dY, axis = 0)\n",
        "print(dB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPwOJW5pkfQ5",
        "outputId": "0dcb7a85-6598-4b0c-9673-2558052e1ec0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2 3]\n",
            " [4 5 6]]\n",
            "[5 7 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Affine 구현은 다음과 같다."
      ],
      "metadata": {
        "id": "h6Kn_hIhlC5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Affine:\n",
        "  def __init__(self, W, b):\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "    self.x = None\n",
        "    self.dW = None\n",
        "    self.db = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.x = x\n",
        "    out = np.dot(x, self.W) + self.b\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = np.dot(dout, self.W.T)\n",
        "    self.dW = np.dot(self.x.T, dout)\n",
        "    self.db = np.sum(dout, axis = 0)\n",
        "\n",
        "    return dx"
      ],
      "metadata": {
        "id": "LBorpX2Mkvk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.6.3 Softmax-with-Loss 계층"
      ],
      "metadata": {
        "id": "zxzIsVzul4i0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 출력층에서 사용하는 소프트맥스 함수는 입력 값을 정규화하여 출력한다.\n",
        "+ 예를 들어 손글씨 숫자 인식에서의 Softmax 계층의 출력은 다음과 같다."
      ],
      "metadata": {
        "id": "aVimXD0Il9q8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1KRtyBih-zXLQFi1-CKerMQA-ceCbzHeL' width = 550 /><br>"
      ],
      "metadata": {
        "id": "XhRTLhUzmGwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Softmax 계층은 입력 값을 정규화(출력의 합이 1이 되도록 변형)하여 출력한다.\n",
        "+ 손글씨 숫자는 가짓수가 10개(10클래스 분류)이므로 Softmax 계층의 입력은 10개가 된다."
      ],
      "metadata": {
        "id": "sPyG6c0emfaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__NOTE__  \n",
        "신경망에서 수행하는 작업은 __학습__과 __추론__ 두 가지이다.   \n",
        "추론할 때는 일반적으로 Softmax 계층을 사용하지 않는다.   \n",
        "위 그림의 신경망은 추론할 때는 마지막 Affine 계층의 출력을 인식 결과로 이용한다.  \n",
        "신경망에서 정규화하지 않은 출력 결과를 __점수__라고 한다.  \n",
        "즉, 신경망 추론에서 답을 하나만 내는 경우에는 가장 높은 점수만 알면 되니 Softmax 계층은 필요없다.  \n",
        "반면, 신경망을 학습할 때는 Softmax 계층이 필요하다."
      ],
      "metadata": {
        "id": "mY6vJ5hPmrUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 소프트맥스 계층을 구현할 때, 손실 함수인 교차 엔트로피 오차도 포함하여  \n",
        "'Softmax-with-Loss' 계층이라는 이름으로 규현한다.\n",
        "+ Softmax-with-Loss 계층의 계산 그래프를 살펴보자."
      ],
      "metadata": {
        "id": "JcLsS3VUoLlg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1oE2-MjZMsnygcOd8a54zFvqPdAhh52Vv' width = 550/><br>"
      ],
      "metadata": {
        "id": "vTlx83P7oZ7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Softmax-with-Loss 계층은 다소 복잡하므로 결과만 제시한다.\n",
        "+ 위 계산 그래프는 다음과 같이 간소화할 수 있다."
      ],
      "metadata": {
        "id": "Yuvcg9tCpYMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1FvIOonDqGwQ7de18w8CEWwQUwduc9mir' width = 550/><br>"
      ],
      "metadata": {
        "id": "PGrCGy0Yrqs2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 위 계산 그래프에서 소프트맥스 함수는 'Softmax' 계층으로,  \n",
        "교차 엔트로피 오차는 'Cross Entropy Error' 계층으로 표기한다.\n",
        "+ 3클래스 분류를 가정하고 이전 계층에서 3개의 입력을 받는다.\n",
        "+ Softmax 계층은 입력 $(a_1, a_2, a_3)$를 정규화하여 $(y_1, y_2, y_3)$을 출력한다.\n",
        "+ Cross Entropy Error 계층은 Softmax의 출력 $(y_1, y_2, y_3)$와 정답 레이블 $(t_1, t_2, t_3)$를 받고,  \n",
        "이 데이터들로부터 손실 L을 출력한다."
      ],
      "metadata": {
        "id": "C3vZWWslsA2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 역전파 결과에 주목하자.\n",
        "+ Softmax 계층의 역전파는 $(y_1 - t_1, y_2 - t_2, y_3 - t_3)$이다.\n",
        "+ $(y_1, y_2, y_3)$는 Softmax 계층의 출력이고, $(t_1, t_2, t_3)$는 정답 레이블이므로 $(y_1 - t_1, y_2 - t_2, y_3 - t_3)$는 Softmax 계층의 출력과 정답 레이블의 차분인 것이다.\n",
        "+ 신경망의 역전파에서는 이 차이인 오차가 앞 계층에 전해지는 것이다."
      ],
      "metadata": {
        "id": "M0PAmCHrs46x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 신경망 학습의 목적은 신경망의 출력이 정답 레이블과 가까워지도록 가중치 매개변수의 값을 조정하는 것이다.\n",
        "+ 신경망의 출력과 정답 레이블의 오차를 효율적으로 앞 계층에 전달해야 한다.\n",
        "+  $(y_1 - t_1, y_2 - t_2, y_3 - t_3)$라는 결과는 Softmax 계층의 출력과 정답 레이블의 차이로,  \n",
        "신경망의 현재 출력과 정답 레이블의 오차를 나타낸다."
      ],
      "metadata": {
        "id": "wBitiAo4tlRk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 구체적인 예를 살펴보자.\n"
      ],
      "metadata": {
        "id": "ZvPx6ue-usvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "정답 레이블이 (0, 1, 0)일 때 Softmax 계층이 (0.3, 0.2, 0.5)를 출력했다."
      ],
      "metadata": {
        "id": "7Zpkp3dpvGcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 정답 인덱스는 1인데 출력에서는 이 때의 확률이 0.2 이므로 신경망은 제대로 인식하지 못하고 있다.\n",
        "+ 이 경우 Softmax 계층의 역전파는 (0.3, -0.8, 0.5)라는 커다란 오차를 전파한다.\n",
        "+ 결과적으로 Softmax 계층의 앞 계층들은 그 큰 오차로부터 큰 깨달음을 얻게 된다."
      ],
      "metadata": {
        "id": "FRHgXDtsvSKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "정답 레이블이 (0, 1, 0)일 때 Softmax 계층이 (0,01, 0.99, 0)을 출력했다."
      ],
      "metadata": {
        "id": "jNhttxXIvi3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 이 경우 Softmax 계층의 역전파가 보내는 오차는 비교적 작은 (0.01, -0.01, 0)이다.\n",
        "+ 앞 계층으로 전달된 오차가 작으므로 학습하는 정도도 작아진다."
      ],
      "metadata": {
        "id": "qLFuCnHAvqH7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Softmax-with-Loss 계층을 구현해보자."
      ],
      "metadata": {
        "id": "yCVDVqI7vzPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(a):\n",
        "  c = np.max(a)\n",
        "  exp_a = np.exp(a - c)\n",
        "  sum_exp_a = np.sum(exp_a)\n",
        "  y = exp_a / sum_exp_a\n",
        "\n",
        "  return y\n",
        "\n",
        "def cross_entropy_error(y, t):\n",
        "  if y.ndim == 1:\n",
        "    t = t.reshape(1, t.size)\n",
        "    y = y.reshape(1, y.size)\n",
        "\n",
        "  batch_size = y.shape[0]\n",
        "  return -np.sum(np.log(y[np.arange(batch_size),t] + 1e-7)) / batch_size"
      ],
      "metadata": {
        "id": "Mzo5e14tw-ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxWithLoss:\n",
        "  def __init__(self):\n",
        "    self.loss = None # 손실\n",
        "    self.y = None # softmax의 출력\n",
        "    self.t = None # 정답 레이블(원-핫 벡터)\n",
        "\n",
        "  def forward(self, x, t):\n",
        "    self.t = t\n",
        "    self.y = softmax(x)\n",
        "    self.loss = cross_entropy_error(self.y, self.t)\n",
        "    return self.loss\n",
        "\n",
        "  def backward(self, dout = 1):\n",
        "    batch_size = self.t.shape[0]\n",
        "    dx = (self.y - self.t) / batch_size\n",
        "\n",
        "    return dx"
      ],
      "metadata": {
        "id": "wzhrOt2gsUBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 이 구현에서 3.5.2와 4.2.4에서 구현한 함수인 softmax()와 cross_entropy_error()를 이용했다."
      ],
      "metadata": {
        "id": "MwjItrc5xMBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.7 오차역전파법 구현하기"
      ],
      "metadata": {
        "id": "1Rbm0h9gxUd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 지금까지 구현한 계층을 조합해서 신경망을 구축해보자."
      ],
      "metadata": {
        "id": "huoHYcQBxXI5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  5.7.1 신경망 학습의 전체 그림"
      ],
      "metadata": {
        "id": "-IE0LSUGxaKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 신경망 학습의 전체 그림을 복습할 겸 신경망 학습의 순서를 살펴보자."
      ],
      "metadata": {
        "id": "t2EOOVTbxzcf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__전제__ <br/>\n",
        "> 신경망에는 적응 가능한 가중치와 편향이 있고,  \n",
        "이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다.  \n",
        "신경망 학습은 4단계로 수행한다.\n",
        "  "
      ],
      "metadata": {
        "id": "EWb7UCdnx57l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__1단계 - 미니배치__\n",
        "> 훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며,  \n",
        "그 미니배치의 손실 함수 값을 줄이는 것이 목표이다."
      ],
      "metadata": {
        "id": "Ygv_PX6HzBnp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__2단계 - 기울기 산출__\n",
        "> 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다.  \n",
        "기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다."
      ],
      "metadata": {
        "id": "tp2dKreCzIQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__3단계 - 매개변수 갱신__\n",
        "> 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다."
      ],
      "metadata": {
        "id": "2K79QcOMzPtm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__4단계 - 반복__\n",
        "> 1~3단계를 반복한다."
      ],
      "metadata": {
        "id": "otkmvQ5Kzacp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 오차역전파법이 등장하는 단계는 2단계인 '기울기 산출'이다.\n",
        "+ 수치 미분을 사용하면 구현은 쉽지만 계산이 오래 걸린다.\n",
        "+ 오차역전파법을 이용하면 느린 수치 미분과 달리 기울기를 효율적이고 빠르게 구할 수 있다."
      ],
      "metadata": {
        "id": "0jkmROEZzfJN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.7.2 오차역전파법을 적용한 신경망 구현하기"
      ],
      "metadata": {
        "id": "_sRYEudpzv06"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 2층 신경망을 TwoLayerNet 클래스로 구현한다.\n",
        "+ 이 클래스의 인스턴스 변수와 메서드를 정리한 표를 살펴보자."
      ],
      "metadata": {
        "id": "vXBodDZKzy-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|인스턴스 변수|설명|\n",
        "|------|---|\n",
        "|params|신경망의 매개변수를 보관하는 딕셔너리 변수|\n",
        "||params['W1']은 1번째 층의 가중치, params['b1']은 1번째 층의 편향|\n",
        "||params['W2']은 2번째 층의 가중치, params['b2']은 2번째 층의 편향|\n",
        "|layer|신경망의 계층을 보관하는 순서가 있는 딕셔너리 변수|\n",
        "||layers['Affine1], layers['Relu1']와 같이 각 계층을 순서대로 유지|\n",
        "|lastLayer|신경망의 마지막 계층|\n",
        "||이 예에서는 SoftmaxWithLoss 계층|"
      ],
      "metadata": {
        "id": "uQUDNZ4d0Ig2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|메서드|설명|\n",
        "|------|---|\n",
        "|_ _ _init_ _ _(self.input_size, hidden_size,|초기화를 수행한다.|\n",
        "|output_size, weight_init_std)|인수는 순서대로 입력층의 뉴런 수, 은닉층의 뉴런 수, |\n",
        "||출력층의 뉴런 수, 가중치 초기화 시 정규분포 스케일|\n",
        "|predict(self, x)|예측(추론)을 수행한다.|\n",
        "||인수 x는 이미지 데이터|\n",
        "|loss(self, x, t)|손실 함수의 값을 구현한다.|\n",
        "||인수 x는 이미지 데이터, t는 정답레이블(아래 칸의 인수들도 마찬가지)|\n",
        "|accuracy(self, x, t)|정확도를 구한다.|\n",
        "|numerical_gradient(self, x, t)|가중치 매개변수의 기울기를 수치 미분 방식으로 구한다.|\n",
        "|gradient(self, x, t)|가중치 매개변수의 기울기를 오차역전파법으로 구한다.|"
      ],
      "metadata": {
        "id": "OyISeNH00o7d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 클래스의 구현이 좀 길지만 앞서 구현한 클래스와 공통된 부분이 많다.\n",
        "+ 계층을 사용함으로써 인식 결과를 얻는 처리 (predict( ))와  \n",
        "기울기를 구하는 처리(gradient( )) 계층의 전파만으로 동작이 이루어지는 것이다."
      ],
      "metadata": {
        "id": "xIUSW9fN1sLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFejk7hN8JLD",
        "outputId": "1ae0414a-06ba-4ef5-dbc7-faf03e01e2d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "import numpy as np\n",
        "sys.path.append(\"/content/drive/MyDrive/deep-learning-from-scratch-master/\")\n",
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "from collections import OrderedDict\n",
        "\n",
        "class TwoLayerNet:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "\n",
        "        self.lastLayer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.lastLayer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "\n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        # 순전파\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # 역전파\n",
        "        dout = 1\n",
        "        dout = self.lastLayer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads"
      ],
      "metadata": {
        "id": "FGQab0h2wpyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 계층 생성 부분과 역전파 부분을 집중해서 보자.\n",
        "+ 신경망의 계층을 OrderedDict에 보관하는데 이는 순서가 있는 딕셔너리이다.\n",
        "+ 순서가 있다는 것은 딕셔너리에 추가한 순서를 기억한다는 것이다.\n",
        "+ 순전파 때는 추가한 순서대로 각 계층의 forward( ) 메서드를 호출하면 처리가 완료된다.\n",
        "+ 마찬가지로 역전파 떄는 계층을 반대 순서로 호출하면 된다.\n",
        "+ Affine 계층과 ReLU 계층이 각자 내부에서 순전파와 역전파를 처리하고 있으니  \n",
        "계층을 올바른 순서로 연결한 다음 순서대로(혹은 역순으로) 호출해주면 끝이다."
      ],
      "metadata": {
        "id": "AFd4AHpc-gy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 신경망의 구성 요소를 '계층'으로 구현한 덕분에 신경망을 쉽게 구축할 수 있다.\n",
        "+ 각 계층 내부에 구현된 순전파와 역전파를 활용해 인식 처리와 학습에 필요한 기울기를 정확하게 구해보자."
      ],
      "metadata": {
        "id": "YZSGoq6X_IqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.7.3 오차역전파법으로 구한 기울기 검증하기"
      ],
      "metadata": {
        "id": "Cz6NZhRP_WoY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 기울기를 구하는 방법에는 수치 미분과 오차역전파법이 있다.\n",
        "+ 오차역전파법은 매개변수가 많아도 효율적으로 계산할 수 있으니 수치 미분 대신해서 사용한다.\n",
        "+ 수치 미분은 오차역전파법을 정확히 구현했는지 확인하는데 필요하다.\n",
        "+ 수치 미분의 결과와 오차역전파법의 결과를 비교하여  \n",
        "오차역전파법을 제대로 구현했는지 검증하는 작업을 __기울기 확인__ 이라고 한다."
      ],
      "metadata": {
        "id": "0UBCSvVe_Z_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 기울기 확인을 구현해보자."
      ],
      "metadata": {
        "id": "tYQuFBeW_2yL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "import numpy as np\n",
        "sys.path.append(\"/content/drive/MyDrive/deep-learning-from-scratch-master/\")\n",
        "from dataset.mnist import load_mnist\n",
        "from ch05.two_layer_net import TwoLayerNet\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "x_batch = x_train[:3]\n",
        "t_batch = t_train[:3]\n",
        "\n",
        "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
        "grad_backprop = network.gradient(x_batch, t_batch)\n",
        "\n",
        "# 각 가중치의 절대 오차의 평균을 구한다.\n",
        "for key in grad_numerical.keys():\n",
        "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
        "    print(key + \":\" + str(diff))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd9fysYP_FS3",
        "outputId": "abea575a-1645-4e24-e621-de2d7f3711d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1:4.4223373414756884e-10\n",
            "b1:2.742574832341471e-09\n",
            "W2:6.174502973545773e-09\n",
            "b2:1.3998274405596645e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 가장 먼저 MNIST 데이터셋을 읽는다.\n",
        "+ 훈련 데이터 일부를 수치 미분으로 구한 기울기와 오차역전파법으로 구한 기울기의 오차를 확인한다.\n",
        "+ 여기서는 각 가중치 매개변수의 차이의 절댓값을 구하고, 이를 평균한 값이 오차가 된다."
      ],
      "metadata": {
        "id": "sP-uJasVA7Pr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 위 결과는 수치 미분과 오차역전파법으로 구한 기울기의 차이가 매우 작다고 말해준다.\n",
        "+ 예를 들어 1번째 층의 편향 오차는 2.6e-09이다.\n",
        "+ 이는 오차역전파법으로 구한 기울기도 올바름이 드러나면서 실수 없이 구현했다고 할 수 있다."
      ],
      "metadata": {
        "id": "c7eDk2R8BHx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__NOTE__\n",
        "> 수치 미분과 오차역전파법의 결과 오차가 0이 되는 일은 드물다.  \n",
        "이는 컴퓨터가 할 수 있는 계산의 정밀도가 유한하기 때문이다.  \n",
        "정밀도의 한계 때문에 오차는 대부분 0이 되지는 않지만  \n",
        "올바르게 구현했다면 0에 아주 가까운 작은 값이 된다."
      ],
      "metadata": {
        "id": "C2pZ-R8aBypX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.7.4 오차역전파법을 사용한 학습 구현하기"
      ],
      "metadata": {
        "id": "-3sfUvcKCGV-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 오차역전파법을 사용한 신경망 학습을 구현해보자."
      ],
      "metadata": {
        "id": "PCEell9hCKNs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(\"/content/drive/MyDrive/deep-learning-from-scratch-master/\")\n",
        "import numpy as np\n",
        "from dataset.mnist import load_mnist\n",
        "from ch05.two_layer_net import TwoLayerNet\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    # 기울기 계산\n",
        "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
        "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)\n",
        "\n",
        "    # 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEw-D5JXAQEZ",
        "outputId": "baeb4525-9d2d-44b2-b4e3-a1c041d7e6ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.11808333333333333 0.1191\n",
            "0.9076333333333333 0.91\n",
            "0.9202666666666667 0.9229\n",
            "0.93515 0.9349\n",
            "0.9439333333333333 0.943\n",
            "0.9498166666666666 0.9495\n",
            "0.9557 0.953\n",
            "0.95755 0.9535\n",
            "0.9634666666666667 0.9597\n",
            "0.9654666666666667 0.96\n",
            "0.9676833333333333 0.9638\n",
            "0.9708833333333333 0.965\n",
            "0.9714333333333334 0.9651\n",
            "0.9738666666666667 0.9668\n",
            "0.9762166666666666 0.9682\n",
            "0.9771 0.9689\n",
            "0.97915 0.9703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.8 정리"
      ],
      "metadata": {
        "id": "9DFWXyxNAbaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 계산 그래프를 이용하면 계산 과정을 시각적으로 파악할 수 있다.\n",
        "+ 계산 그래프의 노드는 국소적 계산으로 구성된다. 국소적 계산을 조합해 전체 계산을 구성한다.\n",
        "+ 계산 그래프의 순전파는 통상의 계산을 수행한다.  \n",
        "+ 계산 그래프의 역전파로는 각 노드의 미분을 구할 수 있다.\n",
        "+ 신경망의 구성 요소를 계층으로 구현하여 기울기를 효율적으로 계산할 수 있다.(오차역전파법)\n",
        "+ 수차 미분과 오차역전파법의 결과를 비교하면 오차역전파법의 구현에 잘못이 없는지 확인할 수 있다.(기울기 확인)"
      ],
      "metadata": {
        "id": "Ed-KzdCVAfLB"
      }
    }
  ]
}