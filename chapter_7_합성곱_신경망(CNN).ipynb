{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPQWqpRxhydq05/oAbmJdA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Junoflows/DeepLearning_From_Scratch1/blob/main/chapter_7_%ED%95%A9%EC%84%B1%EA%B3%B1_%EC%8B%A0%EA%B2%BD%EB%A7%9D(CNN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 7 합성곱 신경망(CNN)"
      ],
      "metadata": {
        "id": "KWIUvUOVZwne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ CNN은 이미지 인식과 음성 인식 등 다양한 곳에서 사용된다.\n",
        "+ 특히 이미지 인식 분야에서 딥러닝을 활용한 기법은 거의 CNN을 기초로 한다."
      ],
      "metadata": {
        "id": "mIMklUWBZ2a2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.1 전체 구조"
      ],
      "metadata": {
        "id": "vjl36QtlaCP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ CNN도 지금까지의 신경망과 같이 계층을 조합하여 만들 수 있고, __합성곱 계층__과 __풀링 계층__이 새롭게 등장한다.\n",
        "+ 지금까지 본 신경망은 인접하는 계층의 모든 뉴런과 결합되어 있었다.\n",
        "+ 이를 __완전 연결__이라고 하며, 완전히 연결된 계층을 __Affine 계층__이라는 이름으로 구현했다.\n",
        "+ Affine 계층을 사용하면, 층이 5개인 완전연결 신경망은 다음과 같이 구현할 수 있다."
      ],
      "metadata": {
        "id": "PTgPq-Osafjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1_WTO8oHrG2-2AX77o6RxQzx0DU9qNseE' width = 550 /><br>"
      ],
      "metadata": {
        "id": "zB_htWXmdG0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 위와 같이 완전연결 신경망은 Affine 계층 뒤에 활성화 함수를 갖는 ReLU 계층(또는 Sigmoid 계층)이 이어진다.\n",
        "+ 위 그림에는 Affine-ReLU 조합이 4개가 쌓였고 마지막 5번째 층은 Affine 계층에 이어 소프트맥스 계층에서 최종 결과를 출력한다."
      ],
      "metadata": {
        "id": "k0tD5jYK9XpB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ CNN의 구조를 살펴보자."
      ],
      "metadata": {
        "id": "KnO6rKUE92d0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1qH6OS3IT-0glbP0FDzJvLfg7EdUjh7s3' width = 550 /><br>"
      ],
      "metadata": {
        "id": "4Ki_-acZ97Jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ CNN에서는 새로운 '합성곱 계층(Conv)'과 '풀링 계층(Pooling)'이 추가된다.\n",
        "+ CNN의 계층은 'Conv-ReLU-(Pooling)' 흐름으로 연결된다. (풀링 계층은 생략되기도 한다.)\n",
        "+ 이는 지금까지의 'Affine-ReLU' 연결이 'Conv-ReLU-(Pooling)'으로 바뀌었다고 생각할 수 있다.\n",
        "+ CNN은 출력에 가까운 층에서는 지금까지의 'Affine-ReLU' 구성을 사용할 수 있다.\n",
        "+ 마지막 출력 계층에서는 'Affine-Softmax' 조합을 그대로 사용한다."
      ],
      "metadata": {
        "id": "tw9zDSlp-H0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2 합성곱 계층"
      ],
      "metadata": {
        "id": "yJ612_m1_WdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ CNN에서는 __패딩__, __스트라이드__ 등 CNN 고유 용어가 등장한다.\n",
        "+ 각 계층 사이에 3차원 데이터같이 입체적인 데이터가 흐른다는 점에서 완전연결 신경망과 다르다.\n",
        "+ CNN에서 사용하는 합성곱 계층의 구조를 살펴보자."
      ],
      "metadata": {
        "id": "bnUf9okY_Yhw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2.1 완전연결 계층의 문제점"
      ],
      "metadata": {
        "id": "5gRguLMd_m_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 완전연결 신경망에서는 Affine 계층을 사용했다.  \n",
        "이는 인접하는 계층의 뉴런이 모두 연결되고 출력의 수는 임의로 정할 수 있었다.\n",
        "+ 완전연결 신경망의 문제점은 데이터의 형상이 무시된다는 것이다.\n",
        "+ 입력데이터가 이미지 데이터인 경우에 이미지는 가로, 세로, 채널(색상)으로 구성된 3차원 데이터이지만  \n",
        "완전연결 계층에 입력할 때는 3차원 데이터를 1차원으로 평탄화해야한다.\n"
      ],
      "metadata": {
        "id": "9ueCYx7-_pT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNIST 데이터셋을 사용할 때는 형상이 (1,28,28)인 이미지를 1줄로 세운 784개의 데이터를 첫 Affine 계층에 입력했다."
      ],
      "metadata": {
        "id": "gpsP-Y2JA81C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 이미지는 3차원 형상이며 여기에는 공간적 정보가 담겨있다.\n",
        "+ 예를 들어 공간적으로 가까운 픽셀은 값이 비슷하거나, RGB의 각 채널은 서로 관련되어 있거나,  \n",
        "거리가 먼 픽셀끼리는 관련이 없는 등 3차원 속에서 의미를 가지는 패턴이 숨어있을 것이다.\n",
        "+ 완전연결 계층은 형상을 무시하고 모든 입력 데이터를 같은 차원의 뉴런으로 취급하여 위와 같은 정보를 살릴 수 없다."
      ],
      "metadata": {
        "id": "VcdrWeJiBFX4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 완전연결 계층과는 다르게 합성공 계층에서는 형상을 유지한다.\n",
        "+ 이미지도 3차원 데이터로 입력받으며 다음 계층으로 3차원 데이터로 전달한다.\n",
        "+ 즉 CNN에서는 이미지처럼 형상을 가진 데이터를 제대로 이해할 수 있다."
      ],
      "metadata": {
        "id": "RmY487eqBiFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ CNN에서는 합성곱 계층의 입출력 데이터를 __특징 맵__이라고도 한다.\n",
        "+ 합성곱 계층의 입력 데이터를 __입력 특징 맵__, 출력 데이터를 __출력 특징 맵__ 이라고 한다.  \n",
        "(입출력 데이터와 특징 맵을 같은 의미로 생각해도 무방하다.)"
      ],
      "metadata": {
        "id": "WAUx5Z4SBsp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2.2 합성곱 연산"
      ],
      "metadata": {
        "id": "yKZvzJaxB90Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 합성곱 연산은 이미지 처리에서 말하는 __필터 연산__에 해당한다."
      ],
      "metadata": {
        "id": "0gmcLpWHCJCA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1TSjVKfhSO6H6FFl7Q-fjOqKJlJDK71N3' width = 550 /><br>"
      ],
      "metadata": {
        "id": "Pio8MxVuCTSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 위와 같이 합성곱 연산은 입력 데이터에 필터를 적용한다.\n",
        "+ 이 예에서 입력 데이터와 필터는 가로, 세로 방향의 형상을 가지고, 입력은 (4,4), 필터는 (3,3), 출력은 (2,2)가 된다.  \n",
        "(필터를 커널이라고도 한다.)\n",
        "+ 합성곱 연산 예에서 계산이 이뤄지는 순서는 다음과 같다."
      ],
      "metadata": {
        "id": "wAEs2r9lCY4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1lLwfNejOIn-LeZpLFOW_g1tc7uHughf-' width = 550/><br>"
      ],
      "metadata": {
        "id": "JKTKbnMqC_0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 합성곱 연산은 필터의 윈도우를 일정 간격으로 이동해가며 입력 데이터에 적용하는데 원도우란 회색 3×3 부분을 말한다.\n",
        "+ 입력과 필터에서 대응하는 원소끼리 곱한 후 총합을 구한고, 그 결과를 출력의 해당 장소에 저장한다.\n",
        "+ 위 과정을 모든 장소에서 수행하면 합성곱 연산의 출력이 완성된다."
      ],
      "metadata": {
        "id": "l4W5tpmhDEc4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 완전연결 신경망에서는 가중치 매개변수와 편향이 존재하는데 CNN에서는 필터의 매개변수가 가중치에 해당한다.\n",
        "+ CNN에 편향까지 포함하면 다음과 같은 흐름이 된다."
      ],
      "metadata": {
        "id": "DwAIHWFLDipd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1Gu12Lv5dkG2JO5qGbL3MIm2-9W9ugggq' width = 550 /><br>"
      ],
      "metadata": {
        "id": "wSbIok1CD094"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 편향은 항상 (1×1)로 필터를 적용한 후의 데이터 모든 원소에 더해진다."
      ],
      "metadata": {
        "id": "NwroQWLcD3jP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2.3 패딩"
      ],
      "metadata": {
        "id": "xjvI9tOqEV0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 합성곱 연산을 수행하기 전에 입력 데이터 주변을 특정 값(0)으로 채우기도 하는데, 이를 __패딩__이라고 한다.\n",
        "+ 예를 들어 아래 그림은 (4,4) 크기의 입력 데이터에 푹이 1인 패딩을 적용한 것이다.\n",
        "+ 폭 1자리 패딩이라 하면 입력 데이터 사방 1픽셀을 특정 값으로 채우는 것이다."
      ],
      "metadata": {
        "id": "zNF2MFhK1vSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1XYs1LSvcGoyLwoww27h4b_G2t5bVSrxl' width = 550 /><br>"
      ],
      "metadata": {
        "id": "qfJCwwHT2ciz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 처음에 크기가 (4,4)인 데이터에 패딩이 추가되어 (6,6)이 된다.\n",
        "+ 위에서는 패딩을 1로 설정했지만 2나 3 등 원하는 정수로 설정할 수 있다.\n",
        "+ 7-5 그림에 패딩을 2로 설정하면 입력 데이터의 크기는 (8,8)이 되고, 3으로 설정하면 (10,10)이 된다."
      ],
      "metadata": {
        "id": "J-sOjrbH2gvl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2.4 스트라이드\n"
      ],
      "metadata": {
        "id": "oDI81U7S3DEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 필터를 적용하는 위치의 간격을 __스트라이드__라고 한다.\n",
        "+ 지금까지의 예는 모두 스트라이드가 1이였지만 2로 설정하면 필터를 적용하는 윈도우가 두 칸씩 이동한다."
      ],
      "metadata": {
        "id": "5wBXLTre3Gnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1nX6kCUHbteiVh_O_-UTqEpO0TwhIbPMa' width = 550 /><br>"
      ],
      "metadata": {
        "id": "pE6h8JSC3Qke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 위 그림에서는 크기가 (7,7)인 입력 데이터에 스트라이드를 2로 설정한 필터를 적용한다.\n",
        "+ 스트라이드를 2로 하니 출력은 (3,3)이 되는 것을 볼 수 있는데, 스트라이드를 키우면 출력 크기는 작아진다.\n",
        "+ 반대로 패딩을 크게 하면 출력이 커진다.\n",
        "+ 이 관계를 수식화하여 패딩, 스트라이드, 출력 크기를 어떻게 계산하는지 살펴보자."
      ],
      "metadata": {
        "id": "CfwVXayi3hL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "입력 크기를 $(H, W)$, 필터 크기를 $(FH, FW)$, 출력 크기를 $(OH, OW)$, 패딩을 $P$, 스트라이드를 $S$라 하면 출력 쿠기는 다음 식으로 계산한다."
      ],
      "metadata": {
        "id": "4V99pVmW344q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large OH = \\frac{H + 2P - FH}{S}+1$ <br/>\n",
        "\n",
        "$\\large OW = \\frac{W + 2P - FW}{S}+1$"
      ],
      "metadata": {
        "id": "oIxWuUQT4OT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 이 식을 사용하여 연습해보자."
      ],
      "metadata": {
        "id": "FvFk0fsGo9Pl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=18hipyAUmHUc2NU7FuXssJoMFQL_EDuKY' width = 550 /><br>"
      ],
      "metadata": {
        "id": "SWngHVZyo_qG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 위 식에 단순히 값을 대입하기만 하면 출력의 크기를 구할 수 있다.\n",
        "+ 단 위 식의 $OH$, $OW$가 정수로 나눠떨어지는 값이어야 한다.\n",
        "+ 출력의 크기가 정수가 아니면 오류를 내거나, 반올림하여 정수로 만드는 등의 방법으로 구현해야 한다."
      ],
      "metadata": {
        "id": "Rkb7vTZWpOhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2.5 3차원 데이터의 합성곱 연산"
      ],
      "metadata": {
        "id": "zZboPt58po53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 2차원 형상의 합성곱을 살펴보았다. 하지만 이미지만 해도 가로, 세로, 채널 3차원 데이터이다.\n",
        "+ 3차원 데이터를 다루는 합성곱 연산을 살펴보자.\n",
        "+ 아래는 3차원 데이터의 합성곱 연산이다."
      ],
      "metadata": {
        "id": "2rPFQELorVcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1y_gAj6jpa6jdfDSyT0F0qgJvwdCLxxAu' width = 550/><br>"
      ],
      "metadata": {
        "id": "YiQT82NUr5EX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 계산 순서는 다음과 같은데, 2차원일 때와 비교하면 채널 방향으로 특징 맵이 늘어났다.\n",
        "+ 채널쪽으로 특징 맵이 여러 개 있다면 입력 데이터와 필터의 합성곱 연산을 채널마다  \n",
        "수행하고 그 결과를 더해서 하나의 출력을 얻는다."
      ],
      "metadata": {
        "id": "2mJ8ug3Nr-C9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1PBfyrs2GPtXZerRUvh3n7poaro-c_IN7' width = 550/><br>"
      ],
      "metadata": {
        "id": "X7hBPmAVsLmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 3차원의 합성곱 연산에서 주의할 점은 입력 데이터와 패널의 채널 수가 같아야 한다는 것이다.\n",
        "+ 필터 자체의 크기는 원하는 값으로 설정할 수 있고 모든 채널의 필터는 같은 크기여야 한다.\n",
        "+ 이 예에서는 필터의 크기가 (3, 3)이지만, (2, 2)나 (1, 1), (5, 5) 등으로 설정해도 된다."
      ],
      "metadata": {
        "id": "zMz7NDwNsQ42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2.6 블록으로 생각하기"
      ],
      "metadata": {
        "id": "Nqfl-V_vs1a-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 3차원의 합성곱 연산은 데이터와 필터를 직육면체 블록이라고 생각하면 된다.\n",
        "+ 3차원 데이터를 다차원 배열로 나타낼 때는 (채널, 높이, 너비) 순으로 쓴다.\n",
        "+ 채널 수 $C$, 높이 $H$, 너비 $W$ 인 데이터의 형상은$(C, H, W)$로 쓴다.\n",
        "+ 필터도 같은데 채널 수 $C$, 높이 $FH$, 너비 $FW$ 인 데이터의 형상은$(C, FH, FW)$로 쓴다."
      ],
      "metadata": {
        "id": "K0Wn6-Wes3mJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1gnffzmF1OknNoI-xeVboJzk2SC3NxKrj' width = 550 /><br>"
      ],
      "metadata": {
        "id": "JkfCEM-lw9dq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 이 예에서 출력 데이터는 한 장의 특징 맵이다. 즉 채널이 1개인 특징 맵이다.\n",
        "+ 합성곱 연산의 출력으로 다수의 채널을 내보내는 방법으로는 필터를 다수 사용하는 것이다."
      ],
      "metadata": {
        "id": "_NmQBme2xZiL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1Z_bP9AlRlVnCR6xtYXqvFvhmkY2qt3vE' width = 550 /><br>"
      ],
      "metadata": {
        "id": "n5uXX_KoyD5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 이 그림과 같이 필터를 $FN$개 적용하면 출력 맵도 $FN$개가 생성된다.\n",
        "+ 이 $FN$개 맵을 모으면 형상이 $(FN, OH, OW)$인 블록이 완성된다.\n",
        "+ 이 완성된 블록을 다음 계층으로 넘기겠다는 것이 CNN의 처리 흐름이다."
      ],
      "metadata": {
        "id": "nKqafgKSyO_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 합성곱 연산에서는 필터의 수도 고려해야 한다.\n",
        "+ 즉 필터의 가중치 데이터는 4차원 데이터이며(출력 채널 수, 입력 채널 수, 높이, 너비) 순으로 쓴다.\n",
        "+ 예를 들어 채널 수3, 크기 5×5인 필터가 20개 있다면 (20, 3, 5, 5)로 쓴다.\n",
        "+ 합성곱 연산에도 완전연결 계층처럼 편향이 쓰이는데 다음은 편향까지 고려한 모습이다."
      ],
      "metadata": {
        "id": "NQ4gVbpxdyRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1AyQOlSCYfxuFQdCAJ4u1lYt_ibtvaTnX' width = 550 /><br>"
      ],
      "metadata": {
        "id": "zYm-lfvyea_l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 편향은 채널 하나에 값 하나씩으로 구성된다.\n",
        "+ 위 예에서 편향의 형상은 $(FN, 1, 1)$이고, 펼터의 출력 결과의 형상은 $(FN, OH, OW)$이다.\n",
        "+ 두 블록을 더하면 편향의 각 값이 필터의 출력인 $(FN, OH, OW)$ 블록의 대응 채널의 원소 모두에 더해진다.\n",
        "+ 형상이 다른 블록의 덧셈은 넘파이의 브로드캐스트 기능으로 쉽게 구현할 수 있다.\n"
      ],
      "metadata": {
        "id": "wzywRSqwjuFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2.7 배치 처리"
      ],
      "metadata": {
        "id": "ex1bHLUVlVKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 신경망 처리에서는 입력 데이터를 한 덩어리로 묶어 배치로 처리했다.\n",
        "+ 완전연결 신경망 구현에서는 이 방식을 지원하여 치리 효율을 높이고, 미니배치 방식의 학습도 지원했다.\n",
        "+ 합성곱 연산도 배치 처리를 지원하고자 하는데 각 계층을 흐르는 데이터의 차원을 하나 늘려 4차원 데이터로 저장한다.\n",
        "+ 데이터를 (데이터 수, 채널 수, 높이, 너비) 순으로 저장하고, 데이터가 N개일 때 배치 처리를 하면 다음과 같다."
      ],
      "metadata": {
        "id": "8BYFGZfBlXij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1aJwDsZAJyQGh1xRI6oUI214OtuB9mzG5' width = 550 /><br>"
      ],
      "metadata": {
        "id": "yN5YRyQomhvd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 위 그림을 보면 각 데이터의 선두에 배치용 차원을 추가했는데 이처럼 데이터는 4차원 형상을 가진 채 각 계층을 타고 흐른다.\n",
        "+ 주의할 점은 신경망에 4차원 데이터가 하나 흐를 때마다 N개에 대한 합성곱 연산이 이뤄진다는 것이다.\n",
        "+ 즉 N회 분의 처리를 한 번에 수행하는 것이다."
      ],
      "metadata": {
        "id": "tf3vAviCmpDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.3 풀링 계층"
      ],
      "metadata": {
        "id": "L6A_-unOm9qc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 풀링은 가로 세로 방향의 공간을 줄이는 연산이다.\n",
        "+ 다음 예는 2×2 영역을 원소 하나로 집약하여 공간의 크기를 줄인다."
      ],
      "metadata": {
        "id": "r-YOOf2dm_WD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1AMGHCvC-2CFeqZn0CqWqCMfqq8lIexD9' width = 550/><br>"
      ],
      "metadata": {
        "id": "Trw8MXAnnSnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 위 그림은 2×2 __최대 풀링__을 스트라이드 2로 처리하는 순서이다.\n",
        "+ 최대 풀링은 최댓값을 구하는 연산으로 '2×2'는 대상 영역의 크기를 뜻한다.\n",
        "+ 즉 2×2 최대 풀링은 2×2 크기의 영역에서 가장 큰 원소 하나를 꺼낸다.\n",
        "+ 스트라이드는 2로 설정했으므로 2×2 윈도우가 웟노 2칸 간격으로 이동한다.\n",
        "+ 일반적으로 풀링의 윈도우 크기와 스트라이드는 같은 값으로 설정한다."
      ],
      "metadata": {
        "id": "LmvnCnOpnUu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3.1 풀링 계층의 특징"
      ],
      "metadata": {
        "id": "0TAmGZSRnv7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__학습해야 할 매개변수가 없다__\n",
        "> 풀링 계층 대상 영역에서 최댓값이나 평균을 취하는 명확한 처리이므로 합성곱 계층과 달리 학습해야 할 매개변수가 없다. <br/>\n",
        "\n",
        "__채널 수가 변하지 않는다__\n",
        "> 풀링 연산은 채널마다 독립적으로 계산하기 때문에 입력 데이터의 채널 수 그대로 출력 데이터로 내보낸다."
      ],
      "metadata": {
        "id": "GHUlMzG5nzMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1qQIKnsjXQImVEq5BvlDvwf3uVZhiaiNN' width = 550/><br>"
      ],
      "metadata": {
        "id": "qucSzbYRooYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__입력의 변화에 영향을 적게 받는다(강건하다)__\n",
        "> 입력 데이터가 조금 변해도 풀링의 결과는 잘 변하지 않는다.  \n",
        "> 아래 예는 입력 데이터의 차이를 풀링이 흡수해 사라지게 하는 모습을 보여준다."
      ],
      "metadata": {
        "id": "MlfAK8wwomjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1Ap_Wq5CxTqdb8bGvpSioF3d9YKyfBL2S' width = 550/><br>"
      ],
      "metadata": {
        "id": "ehE0faOKo_q3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.4 합성곱/풀링 계층 구현하기"
      ],
      "metadata": {
        "id": "icPb98GSpDii"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 합성곱 계층과 풀링 계층을 파이썬으로 구현해보자.\n",
        "+ 오차역전파법에서와 같이 forward와 backward 메서드를 추가하여 모듈로 이용한다.\n",
        "+ 복잡해 보이지만 트릭을 사용하여 쉽게 구현할 수 있는데, 트릭으로 문제를 간단히 하면서 구현해보자."
      ],
      "metadata": {
        "id": "X_rE1d_EpNTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4.1 4차원 배열"
      ],
      "metadata": {
        "id": "CTIBFvkeshKA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ CNN에서 계층 사이를 흐르는 데이터는 4차원이다.\n",
        "+ 예를 들어 데이터의 형상이 (10, 1, 28, 28)이면, 높이 28, 너비 28, 채널 1개인 데이터가 10개인 것이다."
      ],
      "metadata": {
        "id": "7WqACW6Lsj9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x = np.random.rand(10, 1, 28, 28)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTPB4Qk_pAqS",
        "outputId": "17f4ef79-b41d-49e3-e301-14ccaeec2073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 첫 번째 데이터에 접근하려면 x[0]이라고 쓰고 마찬가지로 두 번째 데이터는 x[1] 위치에 있다."
      ],
      "metadata": {
        "id": "qMNwhWlYs8Wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x[0].shape)\n",
        "print(x[1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCMVhxBHtEfw",
        "outputId": "52cb1520-d615-47fc-9642-9036fe2517a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 28, 28)\n",
            "(1, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 첫 번째 데이터의 첫 채널의 공간 데이터에 접근하려면 다음과 같이 적는다."
      ],
      "metadata": {
        "id": "pz0ZnV4atJss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD4z8fsatF8R",
        "outputId": "18430183-ec8b-4ebb-b573-03b8ff30a15d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.09343141, 0.51190193, 0.15219311, 0.11062916, 0.02523548,\n",
              "        0.07989381, 0.83275878, 0.59151531, 0.83071535, 0.28578034,\n",
              "        0.61223078, 0.14402435, 0.78840656, 0.73539505, 0.92902167,\n",
              "        0.70027744, 0.94754291, 0.27490418, 0.91138947, 0.56490294,\n",
              "        0.56253006, 0.09983982, 0.26368652, 0.7744408 , 0.14981987,\n",
              "        0.23599452, 0.35062   , 0.18724918],\n",
              "       [0.8958629 , 0.08234962, 0.99046136, 0.05670391, 0.00515302,\n",
              "        0.56533192, 0.9083201 , 0.76089258, 0.71113001, 0.82750056,\n",
              "        0.14434971, 0.52159395, 0.23106768, 0.86776997, 0.19866956,\n",
              "        0.68667447, 0.15612335, 0.97831695, 0.79498438, 0.79116252,\n",
              "        0.02027393, 0.98603394, 0.6548563 , 0.01995144, 0.52386771,\n",
              "        0.8313716 , 0.15248322, 0.3375214 ],\n",
              "       [0.76968261, 0.58888228, 0.50675021, 0.68858018, 0.59469768,\n",
              "        0.61931724, 0.20568885, 0.72768178, 0.19649778, 0.04767753,\n",
              "        0.1704997 , 0.31454413, 0.77030929, 0.01207481, 0.1256719 ,\n",
              "        0.7970853 , 0.61258452, 0.79815988, 0.09332434, 0.20919989,\n",
              "        0.42927457, 0.43515422, 0.10721821, 0.73410401, 0.17767344,\n",
              "        0.93941939, 0.38601273, 0.75448492],\n",
              "       [0.20742933, 0.25165439, 0.80607745, 0.52644912, 0.40335771,\n",
              "        0.41878379, 0.80405865, 0.27725558, 0.95756381, 0.99611957,\n",
              "        0.67237612, 0.53165641, 0.80225901, 0.1179861 , 0.39556781,\n",
              "        0.34958336, 0.25365677, 0.93061354, 0.17574554, 0.83174694,\n",
              "        0.94175127, 0.8659092 , 0.96585966, 0.85273355, 0.60010419,\n",
              "        0.8261207 , 0.98972687, 0.60111978],\n",
              "       [0.27083399, 0.21936494, 0.57475135, 0.63223559, 0.20998579,\n",
              "        0.40054378, 0.88105261, 0.522993  , 0.04647204, 0.59460571,\n",
              "        0.42375465, 0.50251387, 0.89235497, 0.85676685, 0.51725182,\n",
              "        0.85398669, 0.76166881, 0.50082055, 0.27759284, 0.86177081,\n",
              "        0.51472198, 0.12572934, 0.50743108, 0.23546637, 0.30659263,\n",
              "        0.45045708, 0.74029034, 0.88860376],\n",
              "       [0.08268619, 0.42494723, 0.21506987, 0.92086172, 0.84969244,\n",
              "        0.41292754, 0.92845721, 0.05737338, 0.0304126 , 0.13772389,\n",
              "        0.33427011, 0.23807976, 0.06071725, 0.32053864, 0.42975525,\n",
              "        0.58943594, 0.74688309, 0.05387035, 0.81402055, 0.78170652,\n",
              "        0.92408553, 0.47100292, 0.68786746, 0.93842936, 0.75747393,\n",
              "        0.06944727, 0.42510674, 0.22819366],\n",
              "       [0.81486982, 0.66919749, 0.51688245, 0.22970835, 0.32758676,\n",
              "        0.05787705, 0.56315305, 0.07241565, 0.54923304, 0.75660984,\n",
              "        0.50735918, 0.43859118, 0.03941126, 0.51646787, 0.2834825 ,\n",
              "        0.43979028, 0.62984521, 0.54455296, 0.6740923 , 0.03257287,\n",
              "        0.93448787, 0.16977217, 0.18521571, 0.36176068, 0.79641741,\n",
              "        0.91106376, 0.44851285, 0.00701447],\n",
              "       [0.23541428, 0.55994355, 0.99934699, 0.72514068, 0.21689198,\n",
              "        0.41576814, 0.58202151, 0.82688072, 0.17193367, 0.70879322,\n",
              "        0.39809422, 0.90924341, 0.93551637, 0.03947361, 0.06931645,\n",
              "        0.96164818, 0.71394185, 0.84652213, 0.80354978, 0.55035836,\n",
              "        0.5024372 , 0.78805323, 0.05982502, 0.23073892, 0.28616004,\n",
              "        0.24305017, 0.29446365, 0.50488465],\n",
              "       [0.14349424, 0.38939407, 0.32731123, 0.9547226 , 0.35399689,\n",
              "        0.98039126, 0.55247079, 0.32311191, 0.75117188, 0.99542464,\n",
              "        0.61013637, 0.27064299, 0.46017133, 0.82940005, 0.40922698,\n",
              "        0.08238078, 0.82490563, 0.50481774, 0.49187075, 0.53804983,\n",
              "        0.07282426, 0.5016732 , 0.3692474 , 0.30666045, 0.92354886,\n",
              "        0.93815769, 0.68296158, 0.13215473],\n",
              "       [0.97759032, 0.72627198, 0.47021328, 0.38296123, 0.080529  ,\n",
              "        0.95649849, 0.2046431 , 0.21389009, 0.32215902, 0.33894699,\n",
              "        0.78924666, 0.63932404, 0.08828942, 0.58963275, 0.85989165,\n",
              "        0.52292503, 0.68004393, 0.14337382, 0.56655175, 0.15657082,\n",
              "        0.07207903, 0.8035374 , 0.31082659, 0.97039819, 0.64962597,\n",
              "        0.24171771, 0.21895471, 0.57588872],\n",
              "       [0.41582463, 0.98725602, 0.1249872 , 0.22807378, 0.7030623 ,\n",
              "        0.06144852, 0.42626489, 0.95466082, 0.79232865, 0.25404126,\n",
              "        0.67822191, 0.59635382, 0.28764427, 0.70664133, 0.22783171,\n",
              "        0.38022173, 0.75096008, 0.70899159, 0.18015939, 0.48954221,\n",
              "        0.19045806, 0.34116914, 0.21135486, 0.35107   , 0.42550837,\n",
              "        0.97251994, 0.34742947, 0.46855317],\n",
              "       [0.05562865, 0.38828542, 0.96376939, 0.35969327, 0.4723196 ,\n",
              "        0.17617785, 0.7924855 , 0.57546658, 0.90820583, 0.41042608,\n",
              "        0.87800515, 0.37635043, 0.19514875, 0.90690147, 0.99042195,\n",
              "        0.48913887, 0.96690684, 0.8349537 , 0.97729193, 0.04889457,\n",
              "        0.18823395, 0.707202  , 0.1654386 , 0.45967907, 0.43355191,\n",
              "        0.15494921, 0.13135348, 0.61426991],\n",
              "       [0.04263709, 0.92164039, 0.83760456, 0.42881292, 0.4944635 ,\n",
              "        0.28465222, 0.55934962, 0.47952988, 0.80681259, 0.49772424,\n",
              "        0.10403814, 0.599373  , 0.10890991, 0.39034754, 0.33371551,\n",
              "        0.7166112 , 0.59277936, 0.89724214, 0.22599021, 0.79404176,\n",
              "        0.74539803, 0.42673517, 0.61658232, 0.96622838, 0.19843998,\n",
              "        0.31861431, 0.22666429, 0.51175152],\n",
              "       [0.68064217, 0.99203452, 0.6092739 , 0.84446651, 0.03289954,\n",
              "        0.22810085, 0.0880757 , 0.38992418, 0.66885107, 0.30933379,\n",
              "        0.94965579, 0.07945063, 0.90237112, 0.43319278, 0.44797237,\n",
              "        0.90326607, 0.20523632, 0.98099919, 0.75466164, 0.9436201 ,\n",
              "        0.46392784, 0.32039596, 0.34585819, 0.12218252, 0.58127499,\n",
              "        0.01284976, 0.10267572, 0.54711562],\n",
              "       [0.2723937 , 0.83074091, 0.29990064, 0.97413428, 0.57825068,\n",
              "        0.48213227, 0.43544367, 0.83196054, 0.25877034, 0.47933489,\n",
              "        0.30820895, 0.55163719, 0.77351677, 0.1332032 , 0.47868369,\n",
              "        0.86741164, 0.91509749, 0.68619777, 0.34182851, 0.35658819,\n",
              "        0.82189817, 0.40705168, 0.31967956, 0.61451174, 0.31257055,\n",
              "        0.50273532, 0.36108582, 0.60593089],\n",
              "       [0.45827888, 0.54371317, 0.62448272, 0.69699118, 0.15691309,\n",
              "        0.7697997 , 0.63394742, 0.93113291, 0.91312077, 0.02070504,\n",
              "        0.11608753, 0.88506186, 0.79404861, 0.87049746, 0.36805879,\n",
              "        0.28399392, 0.23143206, 0.32284769, 0.71055617, 0.3893406 ,\n",
              "        0.85039196, 0.05721803, 0.87061597, 0.64123757, 0.84107289,\n",
              "        0.42840262, 0.04418813, 0.39543629],\n",
              "       [0.46396402, 0.62878018, 0.03056871, 0.52278895, 0.15894728,\n",
              "        0.83451067, 0.68635484, 0.5132777 , 0.90447734, 0.50252626,\n",
              "        0.21857294, 0.98351948, 0.5461849 , 0.52563207, 0.77875554,\n",
              "        0.28043566, 0.84653319, 0.79637846, 0.26045415, 0.01452588,\n",
              "        0.90746813, 0.03778402, 0.34481658, 0.31758223, 0.41707625,\n",
              "        0.33085151, 0.0911232 , 0.85395355],\n",
              "       [0.75783985, 0.28297406, 0.34465242, 0.31327761, 0.5640906 ,\n",
              "        0.92195784, 0.75579869, 0.17983516, 0.37161293, 0.29081496,\n",
              "        0.72989901, 0.79707929, 0.2958378 , 0.16363464, 0.96312876,\n",
              "        0.10537275, 0.19981332, 0.64892794, 0.88565622, 0.36599383,\n",
              "        0.62416235, 0.89221364, 0.92863929, 0.74651409, 0.1209363 ,\n",
              "        0.70968517, 0.2986811 , 0.63183788],\n",
              "       [0.95209403, 0.11401765, 0.4958385 , 0.28740867, 0.42163057,\n",
              "        0.60499183, 0.3686108 , 0.14715464, 0.29321715, 0.34471662,\n",
              "        0.0871652 , 0.34581695, 0.04545237, 0.5845861 , 0.38716076,\n",
              "        0.89453762, 0.25971594, 0.29013298, 0.99166128, 0.54417529,\n",
              "        0.96388084, 0.21719478, 0.26266628, 0.35881828, 0.66608024,\n",
              "        0.97429859, 0.19714297, 0.77804135],\n",
              "       [0.14309627, 0.78954816, 0.41955886, 0.17953253, 0.64072058,\n",
              "        0.01423867, 0.73051536, 0.06177945, 0.19046631, 0.37737786,\n",
              "        0.57850833, 0.92605656, 0.65899524, 0.62189987, 0.15026215,\n",
              "        0.52737225, 0.77345208, 0.09673835, 0.61395702, 0.92486407,\n",
              "        0.68134816, 0.67950195, 0.97622934, 0.92163523, 0.60768421,\n",
              "        0.15797088, 0.4161163 , 0.46583671],\n",
              "       [0.29303781, 0.19237608, 0.75413883, 0.2730882 , 0.20804007,\n",
              "        0.99747179, 0.1351194 , 0.36981356, 0.51154825, 0.70137228,\n",
              "        0.80638323, 0.94388388, 0.34764149, 0.37225761, 0.51773797,\n",
              "        0.58927445, 0.19169995, 0.91108963, 0.32877827, 0.28450841,\n",
              "        0.26494199, 0.72618222, 0.90667259, 0.19084991, 0.27910318,\n",
              "        0.2020205 , 0.68225658, 0.75934996],\n",
              "       [0.18275324, 0.46142469, 0.43548404, 0.44899631, 0.74895817,\n",
              "        0.86601645, 0.41055547, 0.26188523, 0.45710232, 0.69139434,\n",
              "        0.42481448, 0.70733386, 0.61281642, 0.06273319, 0.043854  ,\n",
              "        0.67257302, 0.2069975 , 0.49633983, 0.19243955, 0.82603923,\n",
              "        0.17702225, 0.25205608, 0.12118304, 0.33439046, 0.52274748,\n",
              "        0.73758304, 0.83022107, 0.39936996],\n",
              "       [0.68917474, 0.36908162, 0.69275594, 0.35714282, 0.09842333,\n",
              "        0.79591111, 0.71571827, 0.46900723, 0.13501126, 0.67352718,\n",
              "        0.36244451, 0.94030426, 0.92317463, 0.97070444, 0.96107675,\n",
              "        0.56655242, 0.10401757, 0.47224883, 0.08562374, 0.34058696,\n",
              "        0.81041878, 0.21277347, 0.31802752, 0.08282495, 0.51763839,\n",
              "        0.42658902, 0.47361418, 0.97786806],\n",
              "       [0.08965121, 0.74109065, 0.36304149, 0.50740508, 0.40939534,\n",
              "        0.87703418, 0.69220183, 0.00650293, 0.83638604, 0.21512832,\n",
              "        0.27753583, 0.02665501, 0.99885931, 0.77477083, 0.24144235,\n",
              "        0.15711383, 0.01680073, 0.48446742, 0.57635034, 0.40126293,\n",
              "        0.90051042, 0.31488704, 0.24925956, 0.54886621, 0.06451684,\n",
              "        0.23893129, 0.62863283, 0.3063901 ],\n",
              "       [0.24891505, 0.22549553, 0.98253767, 0.54227762, 0.13990746,\n",
              "        0.20656268, 0.39773821, 0.92377625, 0.58226547, 0.12892296,\n",
              "        0.92681154, 0.84230905, 0.33017117, 0.47949152, 0.18732189,\n",
              "        0.56070599, 0.46902496, 0.85285764, 0.37715951, 0.77214086,\n",
              "        0.65056874, 0.75816962, 0.67098502, 0.80282979, 0.34025143,\n",
              "        0.29020227, 0.87819874, 0.58692651],\n",
              "       [0.37490672, 0.45202117, 0.38501814, 0.54365562, 0.81819777,\n",
              "        0.17000267, 0.76491467, 0.90565347, 0.58038985, 0.79359823,\n",
              "        0.77760029, 0.22505635, 0.74329336, 0.94348195, 0.21580978,\n",
              "        0.99513418, 0.31033829, 0.96418299, 0.54041538, 0.89128315,\n",
              "        0.10438946, 0.85816851, 0.65199491, 0.38701684, 0.80590271,\n",
              "        0.96039779, 0.60934865, 0.93841534],\n",
              "       [0.87293903, 0.4554326 , 0.70668848, 0.22182207, 0.1889784 ,\n",
              "        0.29823537, 0.15297611, 0.12969534, 0.34948526, 0.50112991,\n",
              "        0.15377733, 0.05421322, 0.64046372, 0.57192862, 0.01804538,\n",
              "        0.27534853, 0.16937641, 0.08943406, 0.36171152, 0.89085927,\n",
              "        0.80037469, 0.11842357, 0.27475503, 0.99734559, 0.32762901,\n",
              "        0.75530122, 0.76290431, 0.82360986],\n",
              "       [0.61889688, 0.13846161, 0.12317164, 0.59941769, 0.03535706,\n",
              "        0.51233873, 0.65820859, 0.12748499, 0.85667508, 0.40047537,\n",
              "        0.49466951, 0.55886856, 0.54335937, 0.77061117, 0.51386829,\n",
              "        0.11795881, 0.67886929, 0.4002781 , 0.08591634, 0.09137153,\n",
              "        0.19760031, 0.17332712, 0.29801268, 0.59468334, 0.61060784,\n",
              "        0.9728021 , 0.99697847, 0.62306282]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 이처럼 CNN은 4차원 데이터를 다루는데 합성곱 연산의 구현이 복잡해보이지만 'im2col' 이라는 '트릭'이 문제를 단순하게 만들어준다."
      ],
      "metadata": {
        "id": "NCDquPDDtULZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4.2 im2col로 데이터 전개하기"
      ],
      "metadata": {
        "id": "w4TGmVratkFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 합성곱 연산을 구현하려면 for 문을 겹겹히 써야된다고 생각할 수 있다.\n",
        "+ 이는 귀찮고 넘파이의 성능이 떨어진다는 단점이 있다.\n",
        "+ for 문 대신 __im2col__이라는 편의 함수를 사용하여 간단하게 구현해보자."
      ],
      "metadata": {
        "id": "zjchYlN7tnC7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ im2col은 입력 데이터를 필터링하기 좋게 전개하는 함수이다.\n",
        "+ 아래 그림처럼 3차원 입력 데이터에 im2col을 적용하면 2차원 행렬로 바뀐다.\n",
        "+ 정확히는 배치 안의 데이터 수까지 포함한 4차원 데이터를 2차원으로 변환한다."
      ],
      "metadata": {
        "id": "kUWILL0mt4Ca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1F1lHFJ0AGLMJ5BkWJUf-YJdNFJfeT7yx' width = 550 /><br>"
      ],
      "metadata": {
        "id": "AIgp0WDt-CBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ im2col은 필터링하기 좋게 입력 데이터를 전개한다.\n",
        "+ 아래 그림처럼 입력 데이터에서 필터를 적용하는 영역(3차원 블록)을 한 줄로 늘어놓고  \n",
        "이 전개를 필터를 적용하는 모든 영역에서 수행하는 게 im2col이다."
      ],
      "metadata": {
        "id": "fHTm8bd1-V4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=10MP9vkjneE5PADJNQGlchgWBijjDsK-s'  width = 550/><br>"
      ],
      "metadata": {
        "id": "_kv9NSrA-zpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 위 그림에서는 스트라이드를 크게 잡아 필터의 적용 영역이 겹치지 않도록 했지만, 실제 상황에서는 영역이 겹치는 경우가 대부분이다.\n",
        "+ 필터 적용 영역이 겹치면 im2col로 전개한 후의 원소 수가 원래 블록의 원소 수보다 많아진다.\n",
        "+ 즉 im2col을 사용해 구현하면 메모리를 더 많이 소비하는 단점이 있다.\n",
        "\n"
      ],
      "metadata": {
        "id": "hAjNXKJ9_H_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 하지만 컴퓨터는 큰 행렬을 묶어 계산하는데 탁월하기에  \n",
        "문제를 행렬 계산으로 만들면 선형 대수 라이브러리를 활용해 효율을 높일 수 있다."
      ],
      "metadata": {
        "id": "v97tR_9I_hgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__NOTE__\n",
        "> im2col은 'image to column' 즉 이미지에서 행렬로 라는 뜻이다.  \n",
        "카페와 체이너 등의 딥러닝 프레임워크는 im2col이라는 이름의 함수를 만들어  \n",
        "합성곱 계층을 구현할 때 이용하고 있다."
      ],
      "metadata": {
        "id": "CZK6rT7o_t0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ im2col로 입력 데이터를 전개한 다음에 합성곱 계층의 필터를 1열로 전개하고, 두 행렬의 곱을 계싼하면 된다.\n",
        "+ 이는 완전연결 계층의 Affine 계층에서 한 것과 거의 같다."
      ],
      "metadata": {
        "id": "3zIWtQapA_x-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1KOxt_TKeEnSbrhFkj5rdCei6L2NPv7Ax' width = 550/><br>"
      ],
      "metadata": {
        "id": "A4edMOU6BHdw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 이와 같이 im2col 방식으로 출력한 결과는 2차원 행렬이다.\n",
        "+ CNN은 데이터를 4차원 배열로 저장하므로 2차원인 출력 데이터를 4차원으로 변형한다."
      ],
      "metadata": {
        "id": "gQm9It-fBS2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4.3 합성곱 계층 구현하기"
      ],
      "metadata": {
        "id": "GEv696nBBbex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ im2col 함수를 미리 만들어 제공한다. im2col 함수의 인터페이스는 다음과 같다."
      ],
      "metadata": {
        "id": "YnglVaAPBdcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">im2col(input_data, filter_h, filter_w, stride = 1, pad = 0)\n",
        "+ input_data - (데이터 수, 채널 수, 높이, 너비)의 4차원 배열로 이뤄진 입력 데이터\n",
        "+ filter_h - 필터의 높이\n",
        "+ filter_w - 필터의 너비\n",
        "+ stride - 스트라이드\n",
        "+ pad - 패딩\n"
      ],
      "metadata": {
        "id": "Mq6qgiAQBm8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ im2col은 필터 크기, 스트라이드, 패딩을 고려하여 입력 데이터를 2차원 배열로 전개한다.\n",
        "+ im2col을 사용해보자."
      ],
      "metadata": {
        "id": "ylX0T4NcCPC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsERPefyCbF9",
        "outputId": "fb603e0d-b49e-4323-d310-ae32deebfba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os\n",
        "sys.path.append(\"/content/drive/MyDrive/deep-learning-from-scratch-master/\")\n",
        "from common.util import im2col\n",
        "\n",
        "x1 = np.random.rand(1, 3, 7, 7)\n",
        "col1 = im2col(x1, 5, 5, stride = 1, pad = 0)\n",
        "print(col1.shape)\n",
        "\n",
        "x2 = np.random.rand(10, 3, 7, 7)\n",
        "col2 = im2col(x2, 5, 5, stride = 1, pad = 0)\n",
        "print(col2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aszjLuUdtjgK",
        "outputId": "a451be0c-65e9-4d7c-b362-899b3282144c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9, 75)\n",
            "(90, 75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 첫 번째는 배치 크기가 1(데이터 1개), 채널은 3개, 높이·너미가 7×7의 데이터이고 두 번째는 배치 크기만 10이고 나머지는 첫 번째와 같다.\n",
        "+ im2col 함수를 적용한 두 경우 모두 2번째 차원의 원소는 75개이다.\n",
        "+ 이 값은 필터의 원소 수와 같다(채널 3개, 5×5 데이터).\n",
        "+ 배치 크기가 1일 때는 결과의 크기가 (9, 75)이고 10일 때는 10배인 (90, 75) 크기의 데이터가 저장된다."
      ],
      "metadata": {
        "id": "JQ0jAkPPDHtJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ im2col을 사용하여 합성곱 계층을 구현해보자. 합성곱 계층을 Convolution이라는 클래스로 구현한다."
      ],
      "metadata": {
        "id": "7rveuXnPEPfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Convoluion:\n",
        "  def __init__(self, W, b, stride = 1, pad = 0):\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "    self.stride = stride\n",
        "    self.pad = pad\n",
        "\n",
        "  def forward(self, x):\n",
        "    FN, C, FH, FW = self.W.shape\n",
        "    N, C, H, W = x.shape\n",
        "    out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
        "    out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
        "    ###\n",
        "    col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "    col_W = self.W.reshape(FN, -1).T\n",
        "    out = np.dot(col, col_W) + self.b\n",
        "    ###\n",
        "    out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "7aQviV7mDG74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 합성곱 계층은 필터(가중치), 편향, 스트라이드, 패딩을 인수로 받아 초기화한다.\n",
        "+ 필터는 (FN, C, FH, FW)의 4차원 형상이다. 순서대로 필터 개수, 채널, 필터 높이, 필터 너비를 의미한다.\n",
        "+ 입력 데이터를 im2col로 전개하고 필터도 reshape으로 2차원 배열로 전개한 후, 두 행렬의 곱을 구한다.\n",
        "+ 필터를 전개하는 부분은 각 필터 블록을 1줄로 펼쳐 세운다\n",
        "+ 이때 reshape의 두 번째 인수를 -1로 지정했는데 이는 reshape이 제공하는 편의 기능으로  \n",
        " -1을 지정하면 다차원 배열의 원소 수가 변환 후에도 똑같이 유지되도록 적절히 묶어준다."
      ],
      "metadata": {
        "id": "yQ10bba2Fg7D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 예를 들어 (10, 3, 5, 5) 형상의 다차원 배열 W의 언소 수는 750개인데  \n",
        "reshape(10, -1)을 호출하면 750개 원소를 10묶음으로 (10, 75)인 배열로 만들어준다."
      ],
      "metadata": {
        "id": "VjV5923QH5f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ forward 구현의 마지막에서는 출력 데이터를 적절한 형상으로 바꿔준다.\n",
        "+ 이때 transpose 함수를 사용하는데 이는 다차원 배열의 축 순서를 바꿔주는 함수이다.\n",
        "+ 다음과 같이 인덱스를 지정하여 축의 순서를 변경한다."
      ],
      "metadata": {
        "id": "HlkKmECNIMmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1ksgObapRQ_ece2NvAV_mxVsjYF_zSCeg' width = 550 /><br>"
      ],
      "metadata": {
        "id": "cSrMdNJhIZTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 이상 합성곱 계층의 forward 구현으로 im2col로 전개하여 Affine 계층과 거의 똑같이 구현할 수 있다.\n",
        "+ 합성곱 계층의 역전파는 Affine 계층의 구현과 공통점이 많아 따로 설명하지는 않는다.\n",
        "+ 주의할 점은 합성곱 계층의 역전파에서 im2col을 역으로 처리해야 하는데  \n",
        "이는 col2im 함수를 사용하면 된다."
      ],
      "metadata": {
        "id": "VAykAk3wInS2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.4.4 풀링 계층 구현하기"
      ],
      "metadata": {
        "id": "0wXT4A15JIDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 풀링 계층 구현도 합성곱 계층과 마찬가지로 im2col을 사용해 입력 데이터를 전개한다.\n",
        "+ 아래 그림처럼 풀링 적용 영역을 채널마다 독립적으로 전개한다는 점에서 합성곱 계층 때와 다르다."
      ],
      "metadata": {
        "id": "JJy0ITpWiU4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1uJOjemWw6_77ZUlujYhv3L6sDZLNGVq3' width = 550 /><br>"
      ],
      "metadata": {
        "id": "hcXedg7bWN58"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 이렇게 전개한 후, 전개한 행렬에서 행별 최댓값을 구하고 적절한 형상으로 바꿔주면 된다."
      ],
      "metadata": {
        "id": "096HjMJDWRKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1OmIQr94u560wNvAJiQMytH3l-M76bOY8' width = 550 /><br>"
      ],
      "metadata": {
        "id": "3-udiBwZFPDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 이상이 풀링 계층의 forward 처리 흐름이다. 이를 파이썬으로 구현해보자."
      ],
      "metadata": {
        "id": "E-HYDPFeFQs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Pooling:\n",
        "  def __init__(self, pool_h, pool_w, stride = 1, pad = 0):\n",
        "    self.pool_h = pool_h\n",
        "    self.pool_w = pool_w\n",
        "    self.stride = stride\n",
        "    self.pad = pad\n",
        "\n",
        "  def forward(self, x):\n",
        "    N, C, H, W = x.shape\n",
        "    out_h = int(1 + (H - self.poll_h) / self.stride)\n",
        "    out_w = int(1 + (W - self.poll_w) / self.stride)\n",
        "\n",
        "    # 전개(1)\n",
        "    col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "    col = col.reshape(-1, self.pool_h * self.pool_w)\n",
        "\n",
        "    # 최댓값(2)\n",
        "    out = np.max(col, axis = 1)\n",
        "\n",
        "    # 성형(3)\n",
        "    out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "TqPdfm-IFCAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 풀링 계층의 구현은 다음과 같이 3단계로 진행한다.\n",
        "> 1. 입력 데이터를 전개한다.\n",
        "> 2. 행별 최댓값을 구한다.\n",
        "> 3. 적절한 모양으로 성형한다.\n",
        "\n",
        "+ 앞의 코드에서와 같이 각 단계는 한두 줄 정도로 간단히 구현된다."
      ],
      "metadata": {
        "id": "pRyWNXaCGa7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__NOTE__ <br/>\n",
        "최댓값 계산에는 np.max 메서드를 사용할 수 있다. np.max는 인수로 축(axis)을 지정할 수 있는데  \n",
        "이 인수로 지정한 축마다 최댓값을 구할 수 있다.  \n",
        "np.max(x, axis=1)은 입력 x의 1번째 차원의 축마다 최댓값을 구한다."
      ],
      "metadata": {
        "id": "0_yxjmrrGsjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.5 CNN 구현하기"
      ],
      "metadata": {
        "id": "-jCWhd1hHB_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 합성곱 계층과 풀링 계층을 구현했으니 이 계층들을 조합하여 손글씨 숫자를 인식하는 CNN을 구현해보자."
      ],
      "metadata": {
        "id": "PWF1WrTBHRGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1wAYQs0P9xwKPl8anX_eZms5YejTbPhml' width = 550\n",
        "/><br>"
      ],
      "metadata": {
        "id": "GGBEapOGHaq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 위 CNN 네트워크는 'Convolution-ReLU-Pooling_Affine-ReLU-Affine-Softmax' 순으로 흐른다.\n",
        "+ 이를 SimpleConvNet이라는 이름의 클래스로 구현한다.\n",
        "+ SimpleConvNet의 초기화(__init__)를 살펴보자."
      ],
      "metadata": {
        "id": "0ToUvD-mHn9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__초기화 때 받는 인수__\n",
        "+ input_dim - 입력 데이터(채널 수, 높이, 너비)의 차원\n",
        "+ conv_param - 합성곱 계층의 하이퍼파라미터(딕셔너리). 딕셔너리의 키는 다음과 같다.\n",
        "> filter_num - 필터 수  \n",
        "filter_size - 필터 크기  \n",
        "stride - 스트라이드  \n",
        "pad - 패딩  \n",
        "+ hidden_size - 은닉층(완전연결)의 뉴런 수\n",
        "+ output_szie - 출력층(완전연결)의 뉴런 수\n",
        "+ weight_init_std - 초기화 때의 가중치 표준편차"
      ],
      "metadata": {
        "id": "S6Eli0vYH6N9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 합성곱 계층의 하아피퍼라미터는 딕셔너리 형태로 주어진다(conv_param)\n",
        "+ 이는 필요한 하이퍼파라미터의 값이  \n",
        "{'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1} 처럼 저장된다는 뜻이다."
      ],
      "metadata": {
        "id": "ZYK-2cqBItVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleConvNet:\n",
        "      def __init__(self, input_dim=(1, 28, 28),\n",
        "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
        "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "        filter_num = conv_param['filter_num']\n",
        "        filter_size = conv_param['filter_size']\n",
        "        filter_pad = conv_param['pad']\n",
        "        filter_stride = conv_param['stride']\n",
        "        input_size = input_dim[1]\n",
        "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "        self.params['b1'] = np.zeros(filter_num)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
        "        self.params['b2'] = np.zeros(hidden_size)\n",
        "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
        "                                          conv_param['stride'], conv_param['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "      def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "      def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "      def gradient(self, x, t):\n",
        "      # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads"
      ],
      "metadata": {
        "id": "rIOw5gXAxW3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ SimpleConvNet의 초기화는 코드가 길어서 세 부분으로 나눠 설명한다. 다음은 그 중 첫 번째이다."
      ],
      "metadata": {
        "id": "g3vOFQ0eJNM5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "class SimpleConvNet:\n",
        "      def __init__(self, input_dim=(1, 28, 28),\n",
        "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
        "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "        filter_num = conv_param['filter_num']\n",
        "        filter_size = conv_param['filter_size']\n",
        "        filter_pad = conv_param['pad']\n",
        "        filter_stride = conv_param['stride']\n",
        "        input_size = input_dim[1]\n",
        "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "```"
      ],
      "metadata": {
        "id": "xxShn7dZKDnD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 초기화 인수로 주어진 합성곱 계층의 하이퍼파라미터를 딕셔너리에서 꺼낸다.\n",
        "+ 그 후 합성곱 계층의 출력 크기를 계산한다."
      ],
      "metadata": {
        "id": "6JgE88gvJtVS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 다음 코드는 가중치 매개변수를 초기화하는 부분이다.\n",
        "```\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "        self.params['b1'] = np.zeros(filter_num)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
        "        self.params['b2'] = np.zeros(hidden_size)\n",
        "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "```"
      ],
      "metadata": {
        "id": "cYoixoIYKV9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 학습에 필요한 매개변수는 1번째 층의 합성곱 계층과 나머지 두 완전연결 계층의 가중치와 편향이다.\n",
        "+ 이 매개변수들을 인스턴스 변수 params 딕셔너리에 저장한다.\n",
        "+ 1번째 층의 합성곱 계층의 가중치를 W1, 편향을 b1이라는 키로 지정한다.\n",
        "+ 마찬가지로 2번쨰 층의 완전연결 계층의 가중치와 편향을 W2, b2,  \n",
        "마지막 3번째 층의 완전연결 계층의 가중치와편향을 W3, b3라는 키로 각각 저장한다."
      ],
      "metadata": {
        "id": "4hHUreeoKjmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 마지막으로 CNN을 구성하는 계층들을 생성한다.\n",
        "```\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
        "                                           conv_param['stride'], conv_param['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "```"
      ],
      "metadata": {
        "id": "zy9cOcwkK7ly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 순서가 있는 딕셔너리(orderedDict)인 layers에 계층들을 차례로 추가한다.\n",
        "+ 마지막 SoftmaxWithLoss 계층만큼은 last_layer라는 별도 변수에 저장해둔다.\n",
        "+ 이상이 SimpleConvNet의 초기화이다."
      ],
      "metadata": {
        "id": "iAdb9v21Lnpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 이 후 추론을 수행하는 predict 메서드와 손실 함수의 값을 구하는 loss 메서드를 다음과 같이 구현한다.\n",
        "```\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.last_layer.forward(y, t)\n",
        "```"
      ],
      "metadata": {
        "id": "ZUxuy0-xL2aS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 인수 x는 입력 데이터, t는 정답 레이블이다.\n",
        "+ predict 메서드는 초기화 때 layers에 추가한 계층을 맨 앞부터 차례로 forward 메서드를 호출하며  \n",
        "그 결과를 다음 계층에 전달한다.\n",
        "+ loss 메서드는 predict 메서드의 결과를 인수로 마지막 층의 forward 메서드를 호출한다.\n",
        "+ 즉 첫 계층부터 마지막 계층까지 forward를 처리한다."
      ],
      "metadata": {
        "id": "R4utJzbQMVGd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 오차역전파법으로 기울기를 구하는 구현은 다음과 같다.\n",
        "```\n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n",
        "```"
      ],
      "metadata": {
        "id": "O2ba62RCM5PR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 매개변수의 기울기는 오차역전파법으로 구한다. 이 과정은 순전파와 역전파를 반복한다.\n",
        "+ 지금까지 각 계층의 순전파와 역전파 기능을 구현한 것을 적절한 순서로 호출하면 된다.\n",
        "+ 마지막으로 grads 라는 딕셔너리 변수에 각 가중치 매개변수의 기울기를 저장한다."
      ],
      "metadata": {
        "id": "okvslNDiNNBU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 이 SimpleConvNet으로 MNIST 데이터셋을 학습해보자."
      ],
      "metadata": {
        "id": "f5UsC2WGNgu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(\"/content/drive/MyDrive/deep-learning-from-scratch-master/\")  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ch07.simple_convnet import SimpleConvNet\n",
        "from dataset.mnist import load_mnist\n",
        "from common.trainer import Trainer\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
        "x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28),\n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# 매개변수 보존\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "# 그래프 그리기\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-D-R4W_qTQgW",
        "outputId": "a405d9d6-cdf5-40de-8b72-6f1c13c8536f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss:2.299868108283997\n",
            "=== epoch:1, train acc:0.1, test acc:0.094 ===\n",
            "train loss:2.2975427873120347\n",
            "train loss:2.2936226027385826\n",
            "train loss:2.2865980894505125\n",
            "train loss:2.2806804764291275\n",
            "train loss:2.264792333992268\n",
            "train loss:2.248452645388653\n",
            "train loss:2.2288187549924814\n",
            "train loss:2.207505232615164\n",
            "train loss:2.1797330073081986\n",
            "train loss:2.149095658843682\n",
            "train loss:2.0843182961733007\n",
            "train loss:2.0128768528823313\n",
            "train loss:1.977657081899893\n",
            "train loss:1.8529378345018426\n",
            "train loss:1.8489918115398467\n",
            "train loss:1.7496426598977834\n",
            "train loss:1.676484968732113\n",
            "train loss:1.6860743294444933\n",
            "train loss:1.537357123500527\n",
            "train loss:1.4285717741023356\n",
            "train loss:1.3617479888478483\n",
            "train loss:1.310621623613755\n",
            "train loss:1.2909926741092845\n",
            "train loss:1.2069837948747442\n",
            "train loss:1.0385145875223636\n",
            "train loss:1.1606587594807922\n",
            "train loss:0.985056182500618\n",
            "train loss:0.8582292292367952\n",
            "train loss:0.8683125259410742\n",
            "train loss:0.8098039523028234\n",
            "train loss:0.7480469216859761\n",
            "train loss:0.6644059129979583\n",
            "train loss:0.760146176253881\n",
            "train loss:0.6055877795575503\n",
            "train loss:0.5641341974346114\n",
            "train loss:0.6562012469116314\n",
            "train loss:0.6012230925424474\n",
            "train loss:0.6766872393606153\n",
            "train loss:0.7023693368347562\n",
            "train loss:0.7407176802082787\n",
            "train loss:0.48503060585832414\n",
            "train loss:0.6181393835581235\n",
            "train loss:0.5042303418199162\n",
            "train loss:0.40033650140959615\n",
            "train loss:0.45759433821814555\n",
            "train loss:0.5698983244114588\n",
            "train loss:0.44537618473563034\n",
            "train loss:0.49347638461162274\n",
            "train loss:0.3869562445153145\n",
            "train loss:0.4038099330260546\n",
            "=== epoch:2, train acc:0.82, test acc:0.807 ===\n",
            "train loss:0.39669545911053516\n",
            "train loss:0.6802264748018402\n",
            "train loss:0.44977169455350846\n",
            "train loss:0.7368468252740318\n",
            "train loss:0.39544785234278534\n",
            "train loss:0.5299335240441557\n",
            "train loss:0.4972746718495995\n",
            "train loss:0.45726626883667604\n",
            "train loss:0.4241800287782473\n",
            "train loss:0.46341093915877224\n",
            "train loss:0.5229720646951932\n",
            "train loss:0.39497400530537297\n",
            "train loss:0.4930499620965303\n",
            "train loss:0.4449935949460456\n",
            "train loss:0.4405122410543386\n",
            "train loss:0.35539388409116596\n",
            "train loss:0.4807756422607206\n",
            "train loss:0.24196022838289646\n",
            "train loss:0.36576951423543286\n",
            "train loss:0.390216087570334\n",
            "train loss:0.3435567110126951\n",
            "train loss:0.3000751909006327\n",
            "train loss:0.2546066379672837\n",
            "train loss:0.22310279563376653\n",
            "train loss:0.44726734883551456\n",
            "train loss:0.31653011574284184\n",
            "train loss:0.33999888986087135\n",
            "train loss:0.3072161740337917\n",
            "train loss:0.35984761564539275\n",
            "train loss:0.3201919133828405\n",
            "train loss:0.4346950090266089\n",
            "train loss:0.3357336268374384\n",
            "train loss:0.4119454639547353\n",
            "train loss:0.3178107012215472\n",
            "train loss:0.3094819299374924\n",
            "train loss:0.45129683698862577\n",
            "train loss:0.30948244648115897\n",
            "train loss:0.4092533386576563\n",
            "train loss:0.3297895246790249\n",
            "train loss:0.27779900921491985\n",
            "train loss:0.33155941786950444\n",
            "train loss:0.2608517315949657\n",
            "train loss:0.34543740996057076\n",
            "train loss:0.3208824795789089\n",
            "train loss:0.23373284230211186\n",
            "train loss:0.3079619165353887\n",
            "train loss:0.36635730824210716\n",
            "train loss:0.4403778213486342\n",
            "train loss:0.3815368412589341\n",
            "train loss:0.3947804861779447\n",
            "=== epoch:3, train acc:0.875, test acc:0.87 ===\n",
            "train loss:0.3432043330122202\n",
            "train loss:0.2639139037371241\n",
            "train loss:0.34901459855098893\n",
            "train loss:0.2652872629267564\n",
            "train loss:0.32197131598477063\n",
            "train loss:0.30753651874813526\n",
            "train loss:0.2941017989060539\n",
            "train loss:0.46752605348699083\n",
            "train loss:0.27445021981085593\n",
            "train loss:0.26518447579133225\n",
            "train loss:0.3733738072734399\n",
            "train loss:0.19168956727965888\n",
            "train loss:0.3844991821101296\n",
            "train loss:0.25312530039420694\n",
            "train loss:0.21620241722893815\n",
            "train loss:0.3678164535107324\n",
            "train loss:0.19358785437973758\n",
            "train loss:0.19319060019658388\n",
            "train loss:0.32300535549199394\n",
            "train loss:0.5283078945266158\n",
            "train loss:0.23967961394851273\n",
            "train loss:0.3118822381532366\n",
            "train loss:0.2835930126474718\n",
            "train loss:0.44328271995418866\n",
            "train loss:0.34691660668983254\n",
            "train loss:0.2947370818041888\n",
            "train loss:0.31724947875756965\n",
            "train loss:0.3466385273103257\n",
            "train loss:0.328820942081152\n",
            "train loss:0.2485037406166288\n",
            "train loss:0.22558692588887694\n",
            "train loss:0.4785082886575509\n",
            "train loss:0.19445407492918992\n",
            "train loss:0.3236336864944992\n",
            "train loss:0.35164574194152015\n",
            "train loss:0.2832633779506648\n",
            "train loss:0.24281186148337866\n",
            "train loss:0.17852513127003733\n",
            "train loss:0.15815870342155475\n",
            "train loss:0.26926155338485347\n",
            "train loss:0.27691821531484195\n",
            "train loss:0.2903728588415277\n",
            "train loss:0.31089187625538844\n",
            "train loss:0.16784485473400734\n",
            "train loss:0.28407321737834335\n",
            "train loss:0.26997700599492125\n",
            "train loss:0.30637210000935106\n",
            "train loss:0.219908391481003\n",
            "train loss:0.24941288439497697\n",
            "train loss:0.3656603036412598\n",
            "=== epoch:4, train acc:0.912, test acc:0.882 ===\n",
            "train loss:0.27566528241523874\n",
            "train loss:0.2458373373022244\n",
            "train loss:0.2711495666106021\n",
            "train loss:0.23937837069152448\n",
            "train loss:0.2870272777479277\n",
            "train loss:0.17601341881734162\n",
            "train loss:0.17466982033372472\n",
            "train loss:0.20037858487382043\n",
            "train loss:0.2078077313872074\n",
            "train loss:0.17938959017798756\n",
            "train loss:0.14654027740742326\n",
            "train loss:0.40376758334368384\n",
            "train loss:0.3276448633978074\n",
            "train loss:0.22324951433678442\n",
            "train loss:0.24462441678772895\n",
            "train loss:0.2881512050373724\n",
            "train loss:0.13818079014557894\n",
            "train loss:0.243039175825684\n",
            "train loss:0.21036768035383266\n",
            "train loss:0.21122462311420948\n",
            "train loss:0.19618905772866202\n",
            "train loss:0.22921315987728683\n",
            "train loss:0.22846742071018575\n",
            "train loss:0.2933142549448905\n",
            "train loss:0.2669119715774558\n",
            "train loss:0.4401966469525117\n",
            "train loss:0.23031604067017727\n",
            "train loss:0.33415742838417783\n",
            "train loss:0.2369819544282496\n",
            "train loss:0.3634169582299139\n",
            "train loss:0.23575333332439247\n",
            "train loss:0.2416771521125067\n",
            "train loss:0.2792029775120291\n",
            "train loss:0.29512144928029554\n",
            "train loss:0.2131574149058427\n",
            "train loss:0.32943087045645136\n",
            "train loss:0.2124226050096336\n",
            "train loss:0.23005419420753406\n",
            "train loss:0.1192904242868958\n",
            "train loss:0.20761963469284297\n",
            "train loss:0.23211475360306075\n",
            "train loss:0.15277072557195231\n",
            "train loss:0.20409728702976068\n",
            "train loss:0.16513647143904223\n",
            "train loss:0.18698481614315118\n",
            "train loss:0.13989783327047073\n",
            "train loss:0.25505028098879445\n",
            "train loss:0.34869450198316415\n",
            "train loss:0.17060750526215238\n",
            "train loss:0.12714462647309824\n",
            "=== epoch:5, train acc:0.925, test acc:0.897 ===\n",
            "train loss:0.2699541790934137\n",
            "train loss:0.3076640648538981\n",
            "train loss:0.31945401002712237\n",
            "train loss:0.17108847640062116\n",
            "train loss:0.21049051297673912\n",
            "train loss:0.14924482345282536\n",
            "train loss:0.1948959111783183\n",
            "train loss:0.1335883121272794\n",
            "train loss:0.1856160004090069\n",
            "train loss:0.22045980026243328\n",
            "train loss:0.22610720650592317\n",
            "train loss:0.2142793633893301\n",
            "train loss:0.09619569150182733\n",
            "train loss:0.34923638894239895\n",
            "train loss:0.2385759658192521\n",
            "train loss:0.1565851627933185\n",
            "train loss:0.1839320984024843\n",
            "train loss:0.16972964800747306\n",
            "train loss:0.2586654255186455\n",
            "train loss:0.20033947093959228\n",
            "train loss:0.294722998423802\n",
            "train loss:0.17649886338537243\n",
            "train loss:0.2539307859326179\n",
            "train loss:0.1856790272291581\n",
            "train loss:0.2085231727450695\n",
            "train loss:0.3122039652572834\n",
            "train loss:0.14983992069730584\n",
            "train loss:0.1832775124275428\n",
            "train loss:0.36396768374322364\n",
            "train loss:0.19257073309186776\n",
            "train loss:0.3287871035419379\n",
            "train loss:0.1739559593208863\n",
            "train loss:0.32436059642865195\n",
            "train loss:0.15278103824630737\n",
            "train loss:0.13161500516713415\n",
            "train loss:0.11857026754625442\n",
            "train loss:0.3841393206383738\n",
            "train loss:0.17792200662415045\n",
            "train loss:0.28348626275405314\n",
            "train loss:0.2189139367182711\n",
            "train loss:0.2177251512846506\n",
            "train loss:0.12458931790908087\n",
            "train loss:0.16878987127385897\n",
            "train loss:0.20257457422054892\n",
            "train loss:0.09233958439855453\n",
            "train loss:0.2680693978298681\n",
            "train loss:0.15072629563606263\n",
            "train loss:0.18200726129666808\n",
            "train loss:0.20135007634503746\n",
            "train loss:0.20841820479968548\n",
            "=== epoch:6, train acc:0.935, test acc:0.91 ===\n",
            "train loss:0.23539955660983822\n",
            "train loss:0.16475545564385527\n",
            "train loss:0.17672272772219474\n",
            "train loss:0.16009895026017845\n",
            "train loss:0.2958159202030684\n",
            "train loss:0.19103597851192666\n",
            "train loss:0.1885553279877019\n",
            "train loss:0.13050947001550786\n",
            "train loss:0.18248040668779947\n",
            "train loss:0.11042610761284914\n",
            "train loss:0.20165145055565478\n",
            "train loss:0.2286912576442586\n",
            "train loss:0.16800972922380872\n",
            "train loss:0.20505178966671125\n",
            "train loss:0.08880268852425098\n",
            "train loss:0.1393302399717078\n",
            "train loss:0.3177965150238865\n",
            "train loss:0.1347985948724151\n",
            "train loss:0.18626709929520716\n",
            "train loss:0.3427360394917583\n",
            "train loss:0.14594477264352762\n",
            "train loss:0.20206379687064135\n",
            "train loss:0.13977915860966225\n",
            "train loss:0.3339394988204641\n",
            "train loss:0.10586131377945288\n",
            "train loss:0.174495635493198\n",
            "train loss:0.24271534564526803\n",
            "train loss:0.18071737479558014\n",
            "train loss:0.21372714900540235\n",
            "train loss:0.13701266852173843\n",
            "train loss:0.10911224494524462\n",
            "train loss:0.10579138063636742\n",
            "train loss:0.18711424747702504\n",
            "train loss:0.08809357018754659\n",
            "train loss:0.2981893817151134\n",
            "train loss:0.1173566005492076\n",
            "train loss:0.12157620287021281\n",
            "train loss:0.1299063164310581\n",
            "train loss:0.11234920549852728\n",
            "train loss:0.07805425502539698\n",
            "train loss:0.1629301097793511\n",
            "train loss:0.08420586273860846\n",
            "train loss:0.13823220389722646\n",
            "train loss:0.3618772967867487\n",
            "train loss:0.1518220949554164\n",
            "train loss:0.1951092215833487\n",
            "train loss:0.18075896104000022\n",
            "train loss:0.20457598975560515\n",
            "train loss:0.19466785401634984\n",
            "train loss:0.1106704751238404\n",
            "=== epoch:7, train acc:0.948, test acc:0.924 ===\n",
            "train loss:0.07100490658509143\n",
            "train loss:0.19227187210986849\n",
            "train loss:0.1530661427912399\n",
            "train loss:0.1398799037506316\n",
            "train loss:0.19848237012167907\n",
            "train loss:0.16781799325018482\n",
            "train loss:0.17953508090561202\n",
            "train loss:0.15367859343938275\n",
            "train loss:0.16820857242165294\n",
            "train loss:0.14454354584285212\n",
            "train loss:0.17740607168193695\n",
            "train loss:0.19146789520779237\n",
            "train loss:0.1097548213312659\n",
            "train loss:0.16303109309876368\n",
            "train loss:0.08968548950209092\n",
            "train loss:0.22860951337178192\n",
            "train loss:0.12145851338235569\n",
            "train loss:0.17689737580749543\n",
            "train loss:0.11719621605769291\n",
            "train loss:0.17021297564244328\n",
            "train loss:0.12479600338784129\n",
            "train loss:0.1084431729104712\n",
            "train loss:0.23941256812612202\n",
            "train loss:0.11204312361700688\n",
            "train loss:0.12930373690653105\n",
            "train loss:0.13431028927926908\n",
            "train loss:0.14376413730730472\n",
            "train loss:0.16351827770424443\n",
            "train loss:0.08729290799056208\n",
            "train loss:0.19029602646671917\n",
            "train loss:0.10229043233134862\n",
            "train loss:0.18678390906271528\n",
            "train loss:0.21539232871307168\n",
            "train loss:0.10775510429289882\n",
            "train loss:0.2764255180230459\n",
            "train loss:0.13067103066699282\n",
            "train loss:0.21473524551157516\n",
            "train loss:0.1675844242549131\n",
            "train loss:0.10047961312711422\n",
            "train loss:0.16211454459381003\n",
            "train loss:0.13180360720439652\n",
            "train loss:0.17241203325333904\n",
            "train loss:0.12318901803471867\n",
            "train loss:0.17468995889962283\n",
            "train loss:0.2228535149188278\n",
            "train loss:0.10085263089466276\n",
            "train loss:0.12772411583293677\n",
            "train loss:0.1914230402557396\n",
            "train loss:0.17783837783873108\n",
            "train loss:0.05649907796560778\n",
            "=== epoch:8, train acc:0.948, test acc:0.933 ===\n",
            "train loss:0.157609490296157\n",
            "train loss:0.1442673315525935\n",
            "train loss:0.2233493441644703\n",
            "train loss:0.21907048221729983\n",
            "train loss:0.14598987659082402\n",
            "train loss:0.23618625065422724\n",
            "train loss:0.0693490020884115\n",
            "train loss:0.0796786691494634\n",
            "train loss:0.0959925567172402\n",
            "train loss:0.11906052851840652\n",
            "train loss:0.1307393830286283\n",
            "train loss:0.24871488049504845\n",
            "train loss:0.10786309418274452\n",
            "train loss:0.07484488536277994\n",
            "train loss:0.12322574237336886\n",
            "train loss:0.06764962747081804\n",
            "train loss:0.3293118716144244\n",
            "train loss:0.093312381414616\n",
            "train loss:0.0964313638132297\n",
            "train loss:0.11063797149167437\n",
            "train loss:0.14802503708247133\n",
            "train loss:0.08752663281097824\n",
            "train loss:0.07687016431384669\n",
            "train loss:0.11319698155749984\n",
            "train loss:0.16958452025950785\n",
            "train loss:0.10110462379909292\n",
            "train loss:0.12073049779930288\n",
            "train loss:0.10970138663509538\n",
            "train loss:0.12826664078123215\n",
            "train loss:0.058375636212347704\n",
            "train loss:0.12559748966948875\n",
            "train loss:0.11398400311521736\n",
            "train loss:0.1116880200715158\n",
            "train loss:0.0910929977396994\n",
            "train loss:0.1754040756465211\n",
            "train loss:0.10408563331006175\n",
            "train loss:0.11657989694259477\n",
            "train loss:0.07143681989708894\n",
            "train loss:0.0936697202473934\n",
            "train loss:0.050934941497802484\n",
            "train loss:0.1063471342608043\n",
            "train loss:0.17029596089972412\n",
            "train loss:0.11782475905219378\n",
            "train loss:0.16288031285816676\n",
            "train loss:0.08492311022274951\n",
            "train loss:0.06701519508779735\n",
            "train loss:0.06665697665998996\n",
            "train loss:0.057093391088169285\n",
            "train loss:0.09343980272785751\n",
            "train loss:0.21694232506233174\n",
            "=== epoch:9, train acc:0.955, test acc:0.934 ===\n",
            "train loss:0.12066289304257768\n",
            "train loss:0.09431228202646028\n",
            "train loss:0.05576155931966209\n",
            "train loss:0.0566699904852755\n",
            "train loss:0.09229788030035267\n",
            "train loss:0.0571438536204947\n",
            "train loss:0.1070684757572069\n",
            "train loss:0.07241856538175488\n",
            "train loss:0.11038262786373161\n",
            "train loss:0.08029352434004344\n",
            "train loss:0.14245982387849526\n",
            "train loss:0.08372906896949686\n",
            "train loss:0.1995546549540721\n",
            "train loss:0.10176612142860739\n",
            "train loss:0.06982703469386772\n",
            "train loss:0.1806632239459488\n",
            "train loss:0.06925303086089513\n",
            "train loss:0.19633887147419102\n",
            "train loss:0.09774593924449183\n",
            "train loss:0.13468434581973784\n",
            "train loss:0.1152983138566823\n",
            "train loss:0.07581139482154348\n",
            "train loss:0.07455814655274787\n",
            "train loss:0.08221288432976373\n",
            "train loss:0.06905481306906656\n",
            "train loss:0.16778744561400882\n",
            "train loss:0.12744092071799776\n",
            "train loss:0.15686504122672315\n",
            "train loss:0.0337540945122597\n",
            "train loss:0.08772470826891723\n",
            "train loss:0.15171562425478477\n",
            "train loss:0.10143490754203807\n",
            "train loss:0.04607977356807592\n",
            "train loss:0.12656608145056292\n",
            "train loss:0.12487763100994681\n",
            "train loss:0.09042537766093173\n",
            "train loss:0.08966556552010055\n",
            "train loss:0.08639902191708748\n",
            "train loss:0.11274910633412454\n",
            "train loss:0.07116596915889584\n",
            "train loss:0.13299340978398896\n",
            "train loss:0.07402034425550717\n",
            "train loss:0.04024950622882642\n",
            "train loss:0.10998510830597986\n",
            "train loss:0.041662356567952766\n",
            "train loss:0.03010830398934059\n",
            "train loss:0.20366323589049493\n",
            "train loss:0.165062129696681\n",
            "train loss:0.06520921816827033\n",
            "train loss:0.08563492041385466\n",
            "=== epoch:10, train acc:0.966, test acc:0.938 ===\n",
            "train loss:0.04606761255529454\n",
            "train loss:0.0685248923726659\n",
            "train loss:0.08931111910366121\n",
            "train loss:0.11655381247360291\n",
            "train loss:0.09212889260056586\n",
            "train loss:0.06773786951368797\n",
            "train loss:0.07584900484850766\n",
            "train loss:0.10039790735620453\n",
            "train loss:0.043418771389750865\n",
            "train loss:0.10636890776370546\n",
            "train loss:0.07136091402847596\n",
            "train loss:0.11338140822573724\n",
            "train loss:0.08816098270586264\n",
            "train loss:0.0359942443670999\n",
            "train loss:0.049001384297342625\n",
            "train loss:0.1297602730919315\n",
            "train loss:0.05043624687905782\n",
            "train loss:0.14505189083444053\n",
            "train loss:0.05077088119508063\n",
            "train loss:0.11662923026350165\n",
            "train loss:0.18998017907035808\n",
            "train loss:0.08248379023638097\n",
            "train loss:0.04333128748104064\n",
            "train loss:0.08472753927297626\n",
            "train loss:0.1428224908935591\n",
            "train loss:0.1375829907478975\n",
            "train loss:0.14116576621840213\n",
            "train loss:0.09099240306625081\n",
            "train loss:0.07274369220693527\n",
            "train loss:0.056872459396922384\n",
            "train loss:0.04754478707188168\n",
            "train loss:0.07312876039306258\n",
            "train loss:0.09874994031796039\n",
            "train loss:0.11099487455508444\n",
            "train loss:0.06267844207161782\n",
            "train loss:0.04863510750508384\n",
            "train loss:0.10584845979474951\n",
            "train loss:0.06331387097246653\n",
            "train loss:0.101868331315209\n",
            "train loss:0.16121769227513535\n",
            "train loss:0.06201128751684076\n",
            "train loss:0.07498740928470807\n",
            "train loss:0.09082933119267231\n",
            "train loss:0.04723698988543196\n",
            "train loss:0.14415172528483994\n",
            "train loss:0.053399173452928\n",
            "train loss:0.10148511232360763\n",
            "train loss:0.060942816105575774\n",
            "train loss:0.04534719672835304\n",
            "train loss:0.0765045267318754\n",
            "=== epoch:11, train acc:0.96, test acc:0.935 ===\n",
            "train loss:0.06183191866781147\n",
            "train loss:0.06829629162168457\n",
            "train loss:0.11979727893109375\n",
            "train loss:0.15163809035723186\n",
            "train loss:0.11157906997994273\n",
            "train loss:0.08815150527469767\n",
            "train loss:0.176226557914397\n",
            "train loss:0.054714232614613764\n",
            "train loss:0.06689132700433825\n",
            "train loss:0.08236049060998242\n",
            "train loss:0.10883885212645714\n",
            "train loss:0.061048598529684285\n",
            "train loss:0.05330156379517043\n",
            "train loss:0.07165800559836716\n",
            "train loss:0.0703231487980654\n",
            "train loss:0.051513106151861804\n",
            "train loss:0.057855316432565695\n",
            "train loss:0.1092253620611375\n",
            "train loss:0.10773188668935772\n",
            "train loss:0.10664802792860353\n",
            "train loss:0.028698707748310303\n",
            "train loss:0.0827299074332776\n",
            "train loss:0.09540908373655073\n",
            "train loss:0.04754669058218277\n",
            "train loss:0.1270605783842158\n",
            "train loss:0.11287658766410047\n",
            "train loss:0.08328859783753695\n",
            "train loss:0.09383614364401216\n",
            "train loss:0.06035312698354913\n",
            "train loss:0.06334798732696548\n",
            "train loss:0.024882265376868287\n",
            "train loss:0.15032029334168479\n",
            "train loss:0.14707241445958752\n",
            "train loss:0.04706702021922848\n",
            "train loss:0.059231580674568324\n",
            "train loss:0.10251235185227645\n",
            "train loss:0.07281456536571787\n",
            "train loss:0.09412940242975573\n",
            "train loss:0.08700599418015399\n",
            "train loss:0.09297176986425473\n",
            "train loss:0.04427741619611681\n",
            "train loss:0.11201775463541135\n",
            "train loss:0.055724621541123724\n",
            "train loss:0.08467755748410248\n",
            "train loss:0.058682108902635734\n",
            "train loss:0.042999526788542394\n",
            "train loss:0.06331396047923836\n",
            "train loss:0.19283968238441188\n",
            "train loss:0.07968076481542971\n",
            "train loss:0.04613675183275895\n",
            "=== epoch:12, train acc:0.974, test acc:0.945 ===\n",
            "train loss:0.033049089703910704\n",
            "train loss:0.03022425530934282\n",
            "train loss:0.023747399201111675\n",
            "train loss:0.11565000509005767\n",
            "train loss:0.07899215793220528\n",
            "train loss:0.038120472374200326\n",
            "train loss:0.09124207323056886\n",
            "train loss:0.07925752011743728\n",
            "train loss:0.02355304628042563\n",
            "train loss:0.06408476459115041\n",
            "train loss:0.04815749347514197\n",
            "train loss:0.043525068900370965\n",
            "train loss:0.06752846154101007\n",
            "train loss:0.1104120793199842\n",
            "train loss:0.09000663960354272\n",
            "train loss:0.08034781190817504\n",
            "train loss:0.062391562599692835\n",
            "train loss:0.0453020667303556\n",
            "train loss:0.1057697309272072\n",
            "train loss:0.04450485430516221\n",
            "train loss:0.10816071020067303\n",
            "train loss:0.03618782159646725\n",
            "train loss:0.08908513870983892\n",
            "train loss:0.13299402927748477\n",
            "train loss:0.04511992096705463\n",
            "train loss:0.04233417143823382\n",
            "train loss:0.061817484721765854\n",
            "train loss:0.04201537690766309\n",
            "train loss:0.11074242370536812\n",
            "train loss:0.05108432227610283\n",
            "train loss:0.06683570671246705\n",
            "train loss:0.049647703686317046\n",
            "train loss:0.11184431740461376\n",
            "train loss:0.1059966724538925\n",
            "train loss:0.04747540366863541\n",
            "train loss:0.08526941246871653\n",
            "train loss:0.07575823869878082\n",
            "train loss:0.0662555721737021\n",
            "train loss:0.033036509750777175\n",
            "train loss:0.08878571435099993\n",
            "train loss:0.03399504537107131\n",
            "train loss:0.09003644302025758\n",
            "train loss:0.11870036481849425\n",
            "train loss:0.03572587956612414\n",
            "train loss:0.038984398749203585\n",
            "train loss:0.038792703060817904\n",
            "train loss:0.05578397558262072\n",
            "train loss:0.1032108452742219\n",
            "train loss:0.11243077830364914\n",
            "train loss:0.033066135799779545\n",
            "=== epoch:13, train acc:0.978, test acc:0.947 ===\n",
            "train loss:0.04222625754604252\n",
            "train loss:0.05167267311451802\n",
            "train loss:0.16765972787116046\n",
            "train loss:0.09116119546782546\n",
            "train loss:0.1436128551739022\n",
            "train loss:0.038223359360714186\n",
            "train loss:0.07444924330594282\n",
            "train loss:0.10693835348214592\n",
            "train loss:0.04799238069698312\n",
            "train loss:0.025617782222167387\n",
            "train loss:0.05483052920447275\n",
            "train loss:0.019053122997877245\n",
            "train loss:0.0508610232717992\n",
            "train loss:0.011333417340448\n",
            "train loss:0.05854007652499914\n",
            "train loss:0.028501804880609317\n",
            "train loss:0.03765966720956901\n",
            "train loss:0.08433177527330514\n",
            "train loss:0.028412893936677612\n",
            "train loss:0.06464050551945212\n",
            "train loss:0.04562577560271194\n",
            "train loss:0.07001767925047782\n",
            "train loss:0.11343775636347038\n",
            "train loss:0.020391990608531495\n",
            "train loss:0.03220854223620305\n",
            "train loss:0.08134917167499928\n",
            "train loss:0.04152540515682897\n",
            "train loss:0.03072601125342886\n",
            "train loss:0.11762774187860126\n",
            "train loss:0.05222987207976728\n",
            "train loss:0.04993411706357686\n",
            "train loss:0.027937561199797196\n",
            "train loss:0.0862171534691025\n",
            "train loss:0.07471931241670494\n",
            "train loss:0.030995565663044546\n",
            "train loss:0.04638652138084047\n",
            "train loss:0.0788419488571884\n",
            "train loss:0.021676386038589943\n",
            "train loss:0.064731439302433\n",
            "train loss:0.0382783192349361\n",
            "train loss:0.1757480820138911\n",
            "train loss:0.030528950707467025\n",
            "train loss:0.048381057935330644\n",
            "train loss:0.04367308230503734\n",
            "train loss:0.06433966850213901\n",
            "train loss:0.0330859583685216\n",
            "train loss:0.037360318774929964\n",
            "train loss:0.1515471809604585\n",
            "train loss:0.07288223189528863\n",
            "train loss:0.05665444087353846\n",
            "=== epoch:14, train acc:0.977, test acc:0.949 ===\n",
            "train loss:0.07710455881438906\n",
            "train loss:0.041922753979366484\n",
            "train loss:0.027000731135619408\n",
            "train loss:0.03035599041065359\n",
            "train loss:0.05619798303525342\n",
            "train loss:0.061476738366044635\n",
            "train loss:0.05234257453436673\n",
            "train loss:0.016447554177153316\n",
            "train loss:0.03855484771808993\n",
            "train loss:0.025093043809005327\n",
            "train loss:0.028677032190161756\n",
            "train loss:0.08926596231162329\n",
            "train loss:0.06847054912664984\n",
            "train loss:0.04430860467675751\n",
            "train loss:0.08802406819023288\n",
            "train loss:0.039379604344394116\n",
            "train loss:0.06201928426562311\n",
            "train loss:0.02685139731781783\n",
            "train loss:0.04612394974978215\n",
            "train loss:0.047938252095482456\n",
            "train loss:0.049871120887221086\n",
            "train loss:0.0788179662710072\n",
            "train loss:0.05905239405622684\n",
            "train loss:0.014129051574428748\n",
            "train loss:0.09172589241840276\n",
            "train loss:0.046104587195293235\n",
            "train loss:0.026372923468235623\n",
            "train loss:0.03483795849893384\n",
            "train loss:0.012644676139411774\n",
            "train loss:0.0575117881087256\n",
            "train loss:0.014141250191670578\n",
            "train loss:0.05195754689479457\n",
            "train loss:0.019761040046855208\n",
            "train loss:0.03425873510110628\n",
            "train loss:0.04230987271187724\n",
            "train loss:0.041960447179654775\n",
            "train loss:0.07998069694192873\n",
            "train loss:0.03156390007426104\n",
            "train loss:0.05908539723101157\n",
            "train loss:0.022391347395457818\n",
            "train loss:0.06024193903626394\n",
            "train loss:0.02717989656559254\n",
            "train loss:0.040524346922322624\n",
            "train loss:0.02286911364193822\n",
            "train loss:0.053885986631331616\n",
            "train loss:0.0382652835566075\n",
            "train loss:0.02556485483338295\n",
            "train loss:0.014146913454358647\n",
            "train loss:0.032368229415682064\n",
            "train loss:0.054645735614571735\n",
            "=== epoch:15, train acc:0.983, test acc:0.955 ===\n",
            "train loss:0.017067335338959128\n",
            "train loss:0.06243413545686888\n",
            "train loss:0.028514407963738025\n",
            "train loss:0.05919508471786854\n",
            "train loss:0.05070888097821308\n",
            "train loss:0.05428376811215313\n",
            "train loss:0.044867035847990226\n",
            "train loss:0.02805232791816712\n",
            "train loss:0.02447614580587356\n",
            "train loss:0.038749806848208064\n",
            "train loss:0.024964902535874224\n",
            "train loss:0.033092441380612216\n",
            "train loss:0.02399276511187444\n",
            "train loss:0.10242780822933006\n",
            "train loss:0.02378843751366355\n",
            "train loss:0.06078292674996879\n",
            "train loss:0.03788496891085579\n",
            "train loss:0.05218413600569323\n",
            "train loss:0.03984295913207232\n",
            "train loss:0.104247780528635\n",
            "train loss:0.012547700159030062\n",
            "train loss:0.05136631894047674\n",
            "train loss:0.02873981363861213\n",
            "train loss:0.05972021049677906\n",
            "train loss:0.023616595742988786\n",
            "train loss:0.06358916242114315\n",
            "train loss:0.05398312459617232\n",
            "train loss:0.04664841230963039\n",
            "train loss:0.01776016804570517\n",
            "train loss:0.017969490115874554\n",
            "train loss:0.04401623466295391\n",
            "train loss:0.012978830626122121\n",
            "train loss:0.011075495380358505\n",
            "train loss:0.014513195928490792\n",
            "train loss:0.020815848245101663\n",
            "train loss:0.015719198394981954\n",
            "train loss:0.03749671165353412\n",
            "train loss:0.05059900256484252\n",
            "train loss:0.05116470247751334\n",
            "train loss:0.027756488856889652\n",
            "train loss:0.012889900493908608\n",
            "train loss:0.008979580261857776\n",
            "train loss:0.033686428080660864\n",
            "train loss:0.06730128622838424\n",
            "train loss:0.031539828492945975\n",
            "train loss:0.019465068618873874\n",
            "train loss:0.028771197696968707\n",
            "train loss:0.014292051102654557\n",
            "train loss:0.07862488076274614\n",
            "train loss:0.06784760961859283\n",
            "=== epoch:16, train acc:0.988, test acc:0.952 ===\n",
            "train loss:0.0790042254391111\n",
            "train loss:0.032730069692149255\n",
            "train loss:0.04927236020400713\n",
            "train loss:0.06898433880730424\n",
            "train loss:0.03741059363485443\n",
            "train loss:0.024673037073725687\n",
            "train loss:0.023733334222364393\n",
            "train loss:0.022648393146994197\n",
            "train loss:0.04368543397689372\n",
            "train loss:0.038728411235790176\n",
            "train loss:0.016477559424689044\n",
            "train loss:0.03316663612134967\n",
            "train loss:0.021876278582625024\n",
            "train loss:0.02576468089114696\n",
            "train loss:0.060581891807377046\n",
            "train loss:0.030953466472042602\n",
            "train loss:0.03688082214749858\n",
            "train loss:0.058209526364439726\n",
            "train loss:0.060887381022384855\n",
            "train loss:0.012667538909601223\n",
            "train loss:0.040883443210762434\n",
            "train loss:0.02469707105740185\n",
            "train loss:0.025685286837485014\n",
            "train loss:0.05431891671996572\n",
            "train loss:0.01877140541206426\n",
            "train loss:0.01799044523777694\n",
            "train loss:0.053443323255816265\n",
            "train loss:0.05008547426125886\n",
            "train loss:0.03674187910836742\n",
            "train loss:0.10035595363836951\n",
            "train loss:0.028401517638242373\n",
            "train loss:0.052289687609817045\n",
            "train loss:0.013604781914178616\n",
            "train loss:0.028200800749692414\n",
            "train loss:0.03151109673802427\n",
            "train loss:0.026021243410341607\n",
            "train loss:0.017027317509628933\n",
            "train loss:0.03321001290032705\n",
            "train loss:0.031129106563993578\n",
            "train loss:0.025303095903818004\n",
            "train loss:0.037927688660152864\n",
            "train loss:0.020961574163315913\n",
            "train loss:0.02945309871542859\n",
            "train loss:0.12036475849079953\n",
            "train loss:0.036160434156284074\n",
            "train loss:0.03882690593817112\n",
            "train loss:0.08327544992423741\n",
            "train loss:0.03193343290418826\n",
            "train loss:0.04609214243841213\n",
            "train loss:0.02555376775723814\n",
            "=== epoch:17, train acc:0.988, test acc:0.953 ===\n",
            "train loss:0.025061946925662923\n",
            "train loss:0.01914947489369422\n",
            "train loss:0.024875827314801445\n",
            "train loss:0.04977637662513218\n",
            "train loss:0.016325927132175243\n",
            "train loss:0.03357819120325335\n",
            "train loss:0.05084430089021828\n",
            "train loss:0.016699588261515966\n",
            "train loss:0.01749634138760203\n",
            "train loss:0.020587167334607425\n",
            "train loss:0.030427881272131852\n",
            "train loss:0.023295196623236324\n",
            "train loss:0.06438664929664256\n",
            "train loss:0.01868540855412322\n",
            "train loss:0.023298198810523155\n",
            "train loss:0.0794919204949894\n",
            "train loss:0.03156063378210074\n",
            "train loss:0.0226012784318151\n",
            "train loss:0.0150852909015212\n",
            "train loss:0.024747998670734364\n",
            "train loss:0.024360311397842928\n",
            "train loss:0.04088440525545422\n",
            "train loss:0.024401534234947855\n",
            "train loss:0.07867383569657706\n",
            "train loss:0.04979143248320019\n",
            "train loss:0.04449626651593206\n",
            "train loss:0.013496927663438976\n",
            "train loss:0.037196823625320855\n",
            "train loss:0.023893326544266003\n",
            "train loss:0.04553621741767843\n",
            "train loss:0.01497170830545518\n",
            "train loss:0.039544322704556906\n",
            "train loss:0.027367568682604645\n",
            "train loss:0.02415500315796529\n",
            "train loss:0.06043104993754614\n",
            "train loss:0.0693750093255568\n",
            "train loss:0.07619239400635813\n",
            "train loss:0.005786086612073316\n",
            "train loss:0.051710454273358054\n",
            "train loss:0.03328349803372063\n",
            "train loss:0.03918110716429421\n",
            "train loss:0.04506960546991863\n",
            "train loss:0.03871744444331751\n",
            "train loss:0.08050132145966434\n",
            "train loss:0.016697003485485366\n",
            "train loss:0.041355588361290295\n",
            "train loss:0.010143843968169263\n",
            "train loss:0.009307353923622056\n",
            "train loss:0.04863853995841536\n",
            "train loss:0.010276449079524212\n",
            "=== epoch:18, train acc:0.987, test acc:0.956 ===\n",
            "train loss:0.06162898761486782\n",
            "train loss:0.016403463688925434\n",
            "train loss:0.0366958799829638\n",
            "train loss:0.01879654870106041\n",
            "train loss:0.027265415464484216\n",
            "train loss:0.01507873422688639\n",
            "train loss:0.03395017952662668\n",
            "train loss:0.01700138278895965\n",
            "train loss:0.019214763464304122\n",
            "train loss:0.02925479682356264\n",
            "train loss:0.03628578808754137\n",
            "train loss:0.03656159949415474\n",
            "train loss:0.0354633706605564\n",
            "train loss:0.01464530021151631\n",
            "train loss:0.020801883835125493\n",
            "train loss:0.009290200120759344\n",
            "train loss:0.04016858327614537\n",
            "train loss:0.04238749129668114\n",
            "train loss:0.027014799623955286\n",
            "train loss:0.02431796276017738\n",
            "train loss:0.026445786862854936\n",
            "train loss:0.016057926276508563\n",
            "train loss:0.015320666263479103\n",
            "train loss:0.04421911117833467\n",
            "train loss:0.01900034131613075\n",
            "train loss:0.0123395273311954\n",
            "train loss:0.020336820792509137\n",
            "train loss:0.0456461083341292\n",
            "train loss:0.02209325841109669\n",
            "train loss:0.031033940663793173\n",
            "train loss:0.0371091719000658\n",
            "train loss:0.03503121301304758\n",
            "train loss:0.03279156584115326\n",
            "train loss:0.008511510311199327\n",
            "train loss:0.013554819835598466\n",
            "train loss:0.06500533322400548\n",
            "train loss:0.05611376922782821\n",
            "train loss:0.07998273354587802\n",
            "train loss:0.031698718319965874\n",
            "train loss:0.026877503018868397\n",
            "train loss:0.02009710707786554\n",
            "train loss:0.016484447815798334\n",
            "train loss:0.024859413285859385\n",
            "train loss:0.016772240906252678\n",
            "train loss:0.040613185176747404\n",
            "train loss:0.029566192580957794\n",
            "train loss:0.019530542996918386\n",
            "train loss:0.015380076523969938\n",
            "train loss:0.018825196807911376\n",
            "train loss:0.042110613370122764\n",
            "=== epoch:19, train acc:0.98, test acc:0.952 ===\n",
            "train loss:0.03031000986184\n",
            "train loss:0.04547326887674738\n",
            "train loss:0.013915526470124997\n",
            "train loss:0.01914877950167955\n",
            "train loss:0.016624297597722694\n",
            "train loss:0.022120230847889322\n",
            "train loss:0.03379260384838346\n",
            "train loss:0.022023716922658695\n",
            "train loss:0.03432709418282835\n",
            "train loss:0.025380967372832478\n",
            "train loss:0.03428824724816876\n",
            "train loss:0.016520455991229355\n",
            "train loss:0.012656597035077027\n",
            "train loss:0.03153366910429395\n",
            "train loss:0.03195614117141567\n",
            "train loss:0.023576133199915503\n",
            "train loss:0.009925149133808287\n",
            "train loss:0.014929363549335056\n",
            "train loss:0.023675765694236683\n",
            "train loss:0.024063758715708365\n",
            "train loss:0.009845917734228144\n",
            "train loss:0.013791666410095436\n",
            "train loss:0.01500115079441587\n",
            "train loss:0.013199880983578067\n",
            "train loss:0.014051170834314947\n",
            "train loss:0.1027283384566894\n",
            "train loss:0.008301620804494224\n",
            "train loss:0.026009764587057522\n",
            "train loss:0.014406656564599997\n",
            "train loss:0.013290069844970491\n",
            "train loss:0.021489705339870317\n",
            "train loss:0.009463728676831974\n",
            "train loss:0.04674025167266315\n",
            "train loss:0.02572566245715572\n",
            "train loss:0.008890211084119304\n",
            "train loss:0.05388862880404053\n",
            "train loss:0.030771757662683804\n",
            "train loss:0.006854538676089665\n",
            "train loss:0.024656241688449248\n",
            "train loss:0.025158857209235167\n",
            "train loss:0.014343786672465324\n",
            "train loss:0.00902727840464939\n",
            "train loss:0.028550101658852745\n",
            "train loss:0.02585024557214442\n",
            "train loss:0.012415241733620842\n",
            "train loss:0.02319976610580525\n",
            "train loss:0.005016310323356687\n",
            "train loss:0.024276010462147654\n",
            "train loss:0.0073853852446694665\n",
            "train loss:0.024245359506179183\n",
            "=== epoch:20, train acc:0.993, test acc:0.958 ===\n",
            "train loss:0.048904696939719036\n",
            "train loss:0.01002589421996249\n",
            "train loss:0.008008880999057255\n",
            "train loss:0.018162641040684758\n",
            "train loss:0.02297251875921071\n",
            "train loss:0.007051922930249258\n",
            "train loss:0.020642186941258217\n",
            "train loss:0.010044168892930146\n",
            "train loss:0.020075838923533647\n",
            "train loss:0.00917621460518706\n",
            "train loss:0.021529553738756757\n",
            "train loss:0.019038758907701615\n",
            "train loss:0.029763017163879794\n",
            "train loss:0.0345694455768039\n",
            "train loss:0.01563797526513527\n",
            "train loss:0.02368376797567728\n",
            "train loss:0.008699276355820653\n",
            "train loss:0.02941495653332779\n",
            "train loss:0.013283870842086433\n",
            "train loss:0.03533670628902842\n",
            "train loss:0.013726805147210266\n",
            "train loss:0.012476020056062844\n",
            "train loss:0.015778142366352974\n",
            "train loss:0.01703513280437294\n",
            "train loss:0.01290825248821044\n",
            "train loss:0.027029474974803822\n",
            "train loss:0.013061293802878536\n",
            "train loss:0.026750111989747017\n",
            "train loss:0.009412704091827085\n",
            "train loss:0.027632380471082775\n",
            "train loss:0.0284813347119167\n",
            "train loss:0.04401926194172459\n",
            "train loss:0.013020491979318434\n",
            "train loss:0.026466974361101365\n",
            "train loss:0.02462544015197457\n",
            "train loss:0.042528768805333496\n",
            "train loss:0.007340637337570708\n",
            "train loss:0.00944508499073773\n",
            "train loss:0.04137201596706935\n",
            "train loss:0.00953367560874649\n",
            "train loss:0.021744991676247216\n",
            "train loss:0.014321034300149381\n",
            "train loss:0.03214952741996176\n",
            "train loss:0.02151031325610949\n",
            "train loss:0.02942561437990978\n",
            "train loss:0.01408164154437575\n",
            "train loss:0.011533071781887035\n",
            "train loss:0.02856266172705008\n",
            "train loss:0.012675466628412622\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.956\n",
            "Saved Network Parameters!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ SimpleConvNet을 MNSIT 데이터셋으로 학습하면 훈련 데이터에 대한 정확도는 99.3%,  \n",
        "시험 데이터에 대한 정확도는 95.6%가 된다.(학습마다 정확도에 약간의 오차가 존재함)\n",
        "+ 시험 데이터에 대한 정확도가 95%라는 것은 비교적 작은 네트워크로서는 아주 높다고 할수 있다."
      ],
      "metadata": {
        "id": "o25N5L7dWi0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.6 CNN 시각화하기"
      ],
      "metadata": {
        "id": "LRyo0G3PUu4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ CNN을 구성하는 합성곱 계층은 입력으로 받은 이미지 데이터에서 무엇을 보고있는 걸까?\n",
        "+ 합성곱 계층을 시각화해서 CNN이 보고 있는 것이 무엇인지 알아보자."
      ],
      "metadata": {
        "id": "uMcV5k0CUwmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.6.1 1번째 층의 가중치 시각화하기"
      ],
      "metadata": {
        "id": "rkDNkFdfU6QN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ MNIST 데이터셋으로 간단한 CNN 학습을 하였는데  \n",
        "1번째 층의 합성곱 계층의 가중치 형상은 (30, 1, 5, 5)였다(필터 30개, 채널 1개, 5×5크기).\n",
        "+ 필터 크기가 5×5, 채널이 1개라는 것은 이 필터를 1채널의 회색조 이미지로 시각화할 수 있다는 것이다.\n",
        "+ 이 합성곱 계층 필터를 이미지로 나타내보자."
      ],
      "metadata": {
        "id": "rHEfAH5VVI4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def filter_show(filters, nx=8, margin=3, scale=10):\n",
        "    \"\"\"\n",
        "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
        "    \"\"\"\n",
        "    FN, C, FH, FW = filters.shape\n",
        "    ny = int(np.ceil(FN / nx))\n",
        "\n",
        "    fig = plt.figure()\n",
        "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "    for i in range(FN):\n",
        "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
        "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "network = SimpleConvNet()\n",
        "print(\"무작위(랜덤) 초기화 후의 가중치 \\n\")\n",
        "filter_show(network.params['W1'])\n",
        "\n",
        "print('학습된 가중치\\n')\n",
        "network.load_params(\"params.pkl\")\n",
        "filter_show(network.params['W1'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tkSSK_FsUwVf",
        "outputId": "e67e339c-427c-4473-89a6-d4e18166929c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "무작위(랜덤) 초기화 후의 가중치 \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 30 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "학습된 가중치\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 30 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1rvJ9QAt8813sYS6_y5v089YdDVbBcmeC' width = 550/><br>"
      ],
      "metadata": {
        "id": "WrwvOQ22V1FX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 위와 같이 학습 전 필터는 무작위로 초기화되고 있어 흑백의 정도에 규칙성이 없다.\n",
        "+ 한편 학습을 마친 필터는 규칙성 있는 이미지가 되었다.\n",
        "+ 흰색에서 검은색으로 점차 변화하는 필터와 덩어리(블롭)가 진 필터 등, 규칙을 띄는 필터로 바뀌었다."
      ],
      "metadata": {
        "id": "aAgMgg2IWTkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 오른쪽 그림같이 규칭성 있는 필터는 무엇을 보고있는 것일까?\n",
        "+ 에지(색상이 바뀐 경계선)와 블롭(국소적으로 덩어리진 영역) 등을 보고 있다.\n",
        "+ 왼쪽 절반이 흰색이고 오른쪽 절반이 검은색일 필터는 세로 방향의 에지에 반응하는 필터이다."
      ],
      "metadata": {
        "id": "JpdiBw61YkSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1ny_9NbsNOooMNpK3RFDsVdS_5YTtrQjg' width = 550 /><br>"
      ],
      "metadata": {
        "id": "JPa2SYtLZGan"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 위 그림은 학습된 필터 2개를 선택하여 입력 이미지에 합성곱 처리를 한 결과이다.\n",
        "+ '필터 1'은 세로 에지에 반응하며 '필터 2'는 가로 에지에 반응하는 것을 알 수 있다.\n",
        "+ 이처럼 합성곱 계층의 필터는 에지나 블롭 등의 원시적인 정보를 추출할 수 있다.  \n",
        "이러한 정보가 뒷 계층에 전달된다는 것이 앞서 구현한 CNN이다."
      ],
      "metadata": {
        "id": "ZBRub9IIZJTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.6.2 층 깊이에 따른 추출 정보 변화"
      ],
      "metadata": {
        "id": "Mmsz0K0JZhDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 1번째 층의 합성곱 계층에서는 에지나 블롭 등의 저수준 정보고 추출된다면,  \n",
        "겹겹이 쌓인 CNN의 각 계층에서는 어떤 정보가 추출될까?\n",
        "+ 딥러닝 시각화에 관한 연구에 따르면 계층이 깊어질수록 추출되는 정보는 더 추상화되는 것을 알 수 있다."
      ],
      "metadata": {
        "id": "e8iXgpmkZi_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 아래 그림은 일반 사물 인식을 수행한 8층 CNN이다.\n",
        "+ 이 네트워크 구조는 AlexNet이라 한다.\n",
        "+ 합성곱 계층과 풀링 계층을 여러 겹 쌓고 마지막으로 완전연결 계층을 거쳐 결과를 출력하는 구조이다.\n",
        "+ 아래 그림에서 블록으로 나타낸 것은 중간 데이터이며, 그 중간 데이터에 합성곱 연산을 연속해서 적용한다."
      ],
      "metadata": {
        "id": "R6uc9FyOZ1Wb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=13ddI2fGUpO7gYvYjGKc88SKp1HfEtb2W' width = 550/><br>"
      ],
      "metadata": {
        "id": "OJmNqKu6aFTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 딥러닝의 흥미로운 점은 합성곱 계층을 여러 겹 쌓으면 층이 깊어지면서 더 복잡하고 추상화된 정보가 추출된다는 것이다.\n",
        "+ 처음 층은 단순한 에지에 반응하고 이어서 텍스처에 반응하고, 더 복잡한 사물의 일부에 반응하도록 변화한다.\n",
        "+ 즉, 층이 깊어지면서 뉴런이 반응하는 대상이 단순한 모양에서 고급정보로 변화해가는데 사물의 의미를 이해하도록 변화하는 것이다."
      ],
      "metadata": {
        "id": "ZfnUxSehaRJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.7 대표적인 CNN"
      ],
      "metadata": {
        "id": "50AH0iI4amiH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ CNN 네트워크의 구성은 다양한데 그 중 중요한 두 가지를 알아보자.\n",
        "+ 하나는 CNN의 원조인 LeNet이고 다른 하나는 딥러닝이 주목받도록 이끈 AlexNet이다."
      ],
      "metadata": {
        "id": "LR1JQ94omSBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.7.1 LeNet"
      ],
      "metadata": {
        "id": "gb8YtbLWmckF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ __LeNet__은 손글씨 숫자를 인식하는 네트워크로, 1998년에 제안되었다.\n",
        "+ 아래 그림처럼 합성곱 계층이 풀링 계층(단순히 원소를 줄이기만 하는 서브샘플링 계층)을 반복하고,  \n",
        "마지막으로 완전연결 계층을 거치면서 결과를 출력한다."
      ],
      "metadata": {
        "id": "vB6tyOWQmcRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1ZXsY8IamHWaaCgb9aRuT5-Z7VHFDAtFL' width = 550 /><br>"
      ],
      "metadata": {
        "id": "l6k0P84Rm_0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ LeNet과 현재의 CNN을 비교하면 몇 가지 면에서 차이가 있다.\n",
        "+ LeNet은 시그모이드 함수를 사용하는 데 반해, 현재는 ReLU를 사용한다.\n",
        "+ LeNet은 서브샘플링을 하여 중간 데이터의 크기를 줄이지만 현재는 최대 풀링이 주류이다."
      ],
      "metadata": {
        "id": "rO_jEuCtnC3g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.7.2 AlexNet"
      ],
      "metadata": {
        "id": "Q-hgTJ0sspvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ LeNet과 비교해 최근인 2012년에 발표된 __AlexNet__은 딥러닝 열풍을 일으키는데 큰 역할을 했다.\n",
        "+ 아래 그림에서 보듯 구성은 기본적으로 LeNet과 크게 다르지 않다."
      ],
      "metadata": {
        "id": "i3ij5KJwu9p3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1OZLzL0r7kAMCle-Ir7RxPz2aZtvcfmeD' width = 550 /><br>"
      ],
      "metadata": {
        "id": "9q0xJHcnvW1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ AlexNet은 합성곱 계층과 풀링 계층을 거듭하며 마지막으로 완전연결 계층을 거쳐 결과를 출력한다.\n",
        "+ LeNet에서 큰 구조는 바뀌지 않지만 다음과 같은 변화를 주었다.\n",
        "> 활성화 함수로 ReLU를 이용한다.  \n",
        "LRN이라는 국소적 정규화를 실시하는 계층을 이용한다.  \n",
        "드롭아웃을 사용한다."
      ],
      "metadata": {
        "id": "VhZNczG3vd2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 네트워크 구성 면에서는 LeNet과 AlexNet에 큰 차이는 없지만, 이를 둘러싼 환경과 컴퓨터 기술이 크게 발달하였다.\n",
        "+ 대량의 데이터를 얻을 수 있고, 대량의 연산을 고속으로 수행할 수 있게 되어 딥러닝 발전에 크게 기여하였다."
      ],
      "metadata": {
        "id": "VBEXzk9Tv00b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__NOTE__ <br/>\n",
        "딥러닝에는 대부분 수많은 매개변수가 쓰여서 학습하기 위해 많은 양의 계산을 해야한다.  \n",
        "GPU와 빅데이터는 이런 문제에 해결책을 던졌다고 말할 수 있다."
      ],
      "metadata": {
        "id": "oXIDLKapwLGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.8 정리"
      ],
      "metadata": {
        "id": "RFhqH0CTwYap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ CNN은 지금까지의 완전연결 계층 네트워크에 합성곱 계층과 풀링 계층을 새로 추가한다.\n",
        "+ 합성곱 계층과 풀링 계층은 im2col(이미지를 행렬로 전개하는 함수)을 이용하면 간단하고 효율적으로 구현할 수 있다.\n",
        "+ CNN을 시각화해보면 계층이 깊어질수록 고급 정보가 추출되는 모습을 확인할 수 있다.\n",
        "+ 대표적인 CNN에는 LeNet과 AlexNet이 있다.\n",
        "+ 딥러닝의 발전에는 빅데이터와 GPU가 크게 기여했다."
      ],
      "metadata": {
        "id": "wR4GIKnVwaGp"
      }
    }
  ]
}